[
  {
    "objectID": "FinalProjectEDA.html",
    "href": "FinalProjectEDA.html",
    "title": "Final Project",
    "section": "",
    "text": "The diabetes binary health indicators file is a subset of 22 variables and 253,690 responses to questions asked of as part of the Behavioral Risk Factor Surveillance System (BHFSS) survey conducted in 2015.\nThe variables on the binary health indicators file are:\n\nHasDiabetes is the target variable that we want to predict. It is a categorical variable with levels “0” for “No diabetes”, “1” for “Pre-diabetes”, and “2”” for “Diabetes”“. This variable has been renamed from the initial name of Diabetes_binary.\nHighBP is a categorical variable with levels “0” for “No high BP” and “1” for “High BP”.\nHighCholesterol is a categorical variable with levels “0” for “No High Cholesterol” and “1” for “High Cholesterol”. This variable has been renamed from the initial name of HighChol.\nCholesterolChecked is a categorical variable with levels “0” for “No cholesterol check in 5 years” and “1” for “Cholesterol check in 5 years”. This variable has been renamed from the initial name of CholCheck.\nBMI is a discrete numeric variable indicating the BMI level of the PT observed.\nSmoker is a factor variable with levels “0” for “No” and “Yes” as responses to the question “Have you smoked at least 100 cigarettes in your life?”.\nStroke is a categorical variable with levels “0” for “No” and “1” for “Yes” as reponses to the question “Have you ever had or been told you had a stroke?”.\nHeartDiseaseorAttack is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Does the PT have coronary heart disease (CHD) or myocardial infraction (MI)?”.\nPhysActivity is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Not including your job, have you participated in physical activity in the past 30 days?”.\nConsumesFruits is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Do you consume fruit 1 or more times per day?”. This variable has been renamed from the initial name of Fruits.\nConsumesVeggies is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Do you consume vegetables 1 or more times per day?”. This variable has been renamed from the initial name of Veggies.\nHeavyAlcoholUse is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “How many drinks do you consume each week?”. Men responding to the question were placed in the “Yes” category if they consumed 14 or more alcoholic drinks in a week, otherwise they were placed in the “No” category. Women responding to the question were placed in the “Yes” category if they consumed 7 or more alcoholic drinks in a week, otherwise they were placed in the “No” category. This variable has been renamed from the initial name of HvyAlcoholConsump.\nHasHealthcare is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Do you have any kind of health care coverage?”. Responses stating the individual had health insurance or prepaid plans were grouped into the “Yes” category. This variable has been renamed from the initial name of AnyHealthcare.\nExpensiveTreatment is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?”. This variable has been renamed from the initial name of NoDocbcCost.\nGeneralHealth is a categorical variable with response levels “1” for “Excellent”, “2” for “Very Good”, “3 for”Good”, “4” for “Fair”, and “5” for “Poor” to the question “Would you say your general health is…?”. This variable has been renamed from the initial name of GenHlth.\nBadMentalHealth is a categorical variable with levels “0” through “30” as responses to the question “Over the last 30 days, how many did you experience poor mental health”. Given the thirty different possible responses, it may be suitable to treat BadMentalHealth as a numeric variable. This variable has been renamed from the initial name of MentHlth.\nBadPhysicalHealth is a categorical variable with levels “0” through “30” as responses to the question “Over the last 30 days, how many did you experience having physical illness or an injury?”. Given the thirty different possible responses, it may be suitable to treat BadPhysicalHealth as a numeric variable. This variable has been renamed from the initial name of PhysHlth.\nDifficultyWalking is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Do you have serious difficulty walking or climbing stairs?”.\nSex is a categorical variable with levels “0” for “Female” and “1” for “Male” to the question “What is your Sex?”.\nAge is a categorical variable with levels ranging from “1” for “18-24” to “13” for “80 or Older” to the question “What is your age?”. The response were binned together in 5year groups with the exception of groups 1 and 13. Group 1 was expanded to account for PTs that were 18yo or 19yo. Group 13 was created to account for anyone who was 80yo or older.\nEducation is a categorical variable with levels “1” through “8” as responses to the question “What is the highest grade or year of school you completed?”. The response levels are “1” for “Never attended school or only Kindergarten”, “2” for “Grades 1 through 8 (Elementary)”, “3” for “Grades 9 through 11 (Some high school)”, “4” for “Grade 12 or GED (High school graduate)”, “5” for “College 1 year to 3 years (Some college or technical school)”, and “6” for “College 4 years or more (College graduate)”.\nIncome is a categorical variable with levels “1” through “8” as reponses to the question “Is your annual income from all sources:?”. The response levels are “1” for “Less than $10,000”, “2” for “Less than $15,000”, “3” for “Less than $20,000”, “4” for “Less than $25,000”, “5” for “Less than $35,000”, “6” for “Less than $50,000”, “7” for “Less than $75,000”, and “8” for “$75,000 or more”.\n\n\n\n\nThe purpose of the Exploratory Data Analysis (EDA) is to summarize the information from the twenty-one predictor variables with respect to Diabetes_binary and determine if any relationships exist.\nThe goal of the EDA is to identify the variables having relationships with Diabetes_binary that are also suitable for prediction modeling. We will use these variables to generate different prediction model types and determine one that has the highest rate of success at predicting if someone has diabetes or not.\n\n\n\n\nBefore we read-in the data set, we need to establish a library required to use the functions required for summary analysis in R.\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(GGally)\nlibrary(ggcorrplot)\n\nNext, we will read-in the data using a relative path to the folder where the data file is stored.\n\n# Use a relative path to import the data. \n\ndiabetes_df &lt;- read.csv(\"./FinalProjectRawData/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\ndiabetes_df &lt;- as_tibble(diabetes_df)\n\ndiabetes_df\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nWe will then check for missingness,\n\nlist(\"Number of missing values in data set\" = sum(is.na(diabetes_df)))\n\n$`Number of missing values in data set`\n[1] 0\n\n\nWe see that there are no missing values throughout the data set so we can proceed with converting the categorical variables to factors and provide meaningful names to variables if needed.\n\n# Convert variables to factors, replace with meaningful names\n\ndiabetes_df &lt;- diabetes_df |&gt;\n  dplyr::rename(\"HasDiabetes\" = Diabetes_binary, \n         \"HighCholesterol\" = HighChol, \n         \"CholesterolChecked\" = CholCheck, \n         \"ConsumesFruits\" = Fruits, \n         \"ConsumesVeggies\" = Veggies, \n         \"HeavyAlcoholUse\" = HvyAlcoholConsump,\n         \"HasHealthcare\" = AnyHealthcare,\n         \"ExpensiveTreatment\" = NoDocbcCost,\n         \"GeneralHealth\" = GenHlth,\n         \"BadMentalHealth\" = MentHlth,\n         \"BadPhysicalHealth\" = PhysHlth,\n         \"DifficultyWalking\" = DiffWalk) |&gt;\n  mutate(across(c(1:4, 6:15, 18:22), as.factor))\n\nstr(diabetes_df)\n\ntibble [253,680 × 22] (S3: tbl_df/tbl/data.frame)\n $ HasDiabetes         : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighCholesterol     : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholesterolChecked  : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 2 2 1 2 1 1 ...\n $ ConsumesFruits      : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 2 2 1 1 2 1 ...\n $ ConsumesVeggies     : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HeavyAlcoholUse     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HasHealthcare       : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ ExpensiveTreatment  : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GeneralHealth       : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 5 3 5 2 2 2 3 3 5 2 ...\n $ BadMentalHealth     : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ BadPhysicalHealth   : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DifficultyWalking   : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Factor w/ 13 levels \"1\",\"2\",\"3\",\"4\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Factor w/ 8 levels \"1\",\"2\",\"3\",\"4\",..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\nReview of our data frame structure shows that we have successfully renamed our variables and converted the categorical variables to factors. It is important to note that while BadMentalHealth and BadPhysicalHealth are categorical responses in the survey, the responses are discrete over a range of possible entries (1 to 30) makes them candidates for numeric analysis.\nNext we will generate a summary of the variables to provide insight on what we can expect to see during our EDA.\n\nprint(list(\"Summary of Diabetes Data Frame Variables\"=summary(diabetes_df)))\n\n$`Summary of Diabetes Data Frame Variables`\n HasDiabetes HighBP     HighCholesterol CholesterolChecked      BMI       \n 0:218334    0:144851   0:146089        0:  9470           Min.   :12.00  \n 1: 35346    1:108829   1:107591        1:244210           1st Qu.:24.00  \n                                                           Median :27.00  \n                                                           Mean   :28.38  \n                                                           3rd Qu.:31.00  \n                                                           Max.   :98.00  \n                                                                          \n Smoker     Stroke     HeartDiseaseorAttack PhysActivity ConsumesFruits\n 0:141257   0:243388   0:229787             0: 61760     0: 92782      \n 1:112423   1: 10292   1: 23893             1:191920     1:160898      \n                                                                       \n                                                                       \n                                                                       \n                                                                       \n                                                                       \n ConsumesVeggies HeavyAlcoholUse HasHealthcare ExpensiveTreatment GeneralHealth\n 0: 47839        0:239424        0: 12417      0:232326           1:45299      \n 1:205841        1: 14256        1:241263      1: 21354           2:89084      \n                                                                  3:75646      \n                                                                  4:31570      \n                                                                  5:12081      \n                                                                               \n                                                                               \n BadMentalHealth  BadPhysicalHealth DifficultyWalking Sex       \n Min.   : 0.000   Min.   : 0.000    0:211005          0:141974  \n 1st Qu.: 0.000   1st Qu.: 0.000    1: 42675          1:111706  \n Median : 0.000   Median : 0.000                                \n Mean   : 3.185   Mean   : 4.242                                \n 3rd Qu.: 2.000   3rd Qu.: 3.000                                \n Max.   :30.000   Max.   :30.000                                \n                                                                \n      Age        Education      Income     \n 9      :33244   1:   174   8      :90385  \n 10     :32194   2:  4043   7      :43219  \n 8      :30832   3:  9478   6      :36470  \n 7      :26314   4: 62750   5      :25883  \n 11     :23533   5: 69910   4      :20135  \n 6      :19819   6:107325   3      :15994  \n (Other):87744              (Other):21594  \n\n\nReeview of the data set summary shows that BMI appears to be centered with outliers that need investigated for removal. With median values of “0” over a range of 1-30, both BadMentalHealth and BadPhysicalHealth have a high tendency response of “0” in the survey. HighBP, HighCholesterol, Smoker, GeneralHealth, Sex and Age each appear to have response ratios near 1:1. The remaining binary variables of CholesterolChecked, Stroke, HeartDiseaseorAttack, PhysActivity, ConsumesFruits, ConsumesVeggies, HeavyAlcoholUse, HasHealthcare, ExpensiveTreatment, and DifficultyWalking each heavily favor one response over the other. The variables of Education and Income both show an increase in responses as the category range values increase.\n\n\n\nThe first variable we will investigate is BMI. Review of the variable summary information shows\n\nprint(list(\"Summary of BMI\"=summary(diabetes_df$BMI)))\n\n$`Summary of BMI`\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   24.00   27.00   28.38   31.00   98.00 \n\n\nthat the data has a Q1 value of 24, Q3 value of 31, Median of 27, and Mean of 28.38. This tells us the data is tightly grouped together with BMI values of less than 13.5 and greater than 41.5 as outliers. These outliers include our Min of 12 and Max of 98. A histogram of BMI shows\n\nBMI_histogram &lt;- ggplot(diabetes_df, aes(BMI)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  labs(title=\"Histogram of BMI\", y=\"Density (# of PTs)\") +\n  scale_x_continuous(breaks = seq(0, 100, by = 10)) +\n  stat_function(fun=dnorm, args=list(mean=mean(diabetes_df$BMI), sd=sd(diabetes_df$BMI))) \n\nBMI_histogram\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nthat the data follows a generally normal distribution with some skewness to the right. The histogram also visually confirms there are outleirs in the BMI ranges of 12-13 and 50-98.\n\nlowrange &lt;- sum(diabetes_df$BMI &lt; 13.5)\n\nhighrange &lt;- sum(diabetes_df$BMI &gt; 41.5)\n\nBMIoutliers &lt;-sum(lowrange, highrange)\n\nprint(list(\"Number of BMI Oultiers\"=BMIoutliers))\n\n$`Number of BMI Oultiers`\n[1] 9847\n\nprint(list(\"BMI Outlier portion of data set\"=BMIoutliers/nrow(diabetes_df)))\n\n$`BMI Outlier portion of data set`\n[1] 0.03881662\n\n\nWe identified there are total of 9847 observation that are outliers when using 1.5 the IQR as our defining statistic. This accounts for over 3.88% of the data set, which is a higher than we’re willing to remove. Because the histogram shows there is close to a 0% probability of entries starting near a BMI of 50,\n\nlowrange &lt;- sum(diabetes_df$BMI &lt; 13.5)\n\nhighrange &lt;- sum(diabetes_df$BMI &gt; 50)\n\nBMIoutliers &lt;-sum(lowrange, highrange)\n\nprint(list(\"Number of BMI Oultiers\"=BMIoutliers))\n\n$`Number of BMI Oultiers`\n[1] 2202\n\nprint(list(\"BMI Outlier portion of data set\"=BMIoutliers/nrow(diabetes_df)))\n\n$`BMI Outlier portion of data set`\n[1] 0.008680227\n\n\nwe udpated the parametrs and found there to be 2202 outlier observations which account for less than 1% of the data set. It would be reasonable to remove these observations from the data set before prediction model training.\nRemoving the outliers then splitting the BMI histogram with respect to HasDiabetes shows\n\ndiabetes_df &lt;- diabetes_df |&gt;\n  filter(BMI &gt;= 13.5) |&gt;\n  filter(BMI &lt;= 50 )\n\nBMI_by_histplot &lt;- ggplot(diabetes_df, aes(BMI)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  labs(title=\"Histogram of BMI by HasDiabetes\", y=\"Density (# of PTs)\") +\n  scale_x_continuous(breaks = seq(0, 100, by = 10)) +\n  stat_function(fun=dnorm, args=list(mean=mean(diabetes_df$BMI), sd=sd(diabetes_df$BMI))) +\n  facet_grid(~HasDiabetes)\n\nBMI_by_histplot\n\n\n\n\n\n\n\n\nthat the distribution of observations in the “No” HasDiabetes group has a center near a BMI of 25 while the “Yes” HasDiabetes group has a center near a BMI of 30. This change of BMI density distributions between the HasDiabetes groups indicates that BMI appears to be a suitable variable for prediction modeling.\nThe next variable we will investigate is HighPB. Review of the variable summary information shows\n\nprint(list(\"Summary of HighBP\"=summary(diabetes_df$HighBP)))\n\n$`Summary of HighBP`\n     0      1 \n143994 107484 \n\n\nthere are greater number of observations of PTs not having high blood pressure to those that do in a ratio of approximately 7:5. This is fairly close to being 1:1, so any differences between the HasDiabetes categories would be due to interactions between the variables.\nUsing a bar plot to observe HighBP with respect to HasDiabetes shows\n\nHighBP_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, HighBP) |&gt;\n  summarize(count = n())\n\nHighPB_plot &lt;- ggplot(HighBP_by, aes(HasDiabetes, count, fill=HighBP)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title = \"Bar Plot of HighBP by HasDiabetes\", x=\"Has Diabetes\", y=\"# of PTs\")\n\nHighPB_plot\n\n\n\n\n\n\n\n\nthat the majority of PTs with diabetes have high blood pressure while the majority of PTs without diabetes do not have high blood pressure. The observations in the “No” HasDiabetes category have a “No” to “Yes” HighBP ratio of approximately 7:4 (near 2:1). The observations in the “Yes” HasDiabetes category have a “No” to “Yes” HighBP ratio of approximately 4:13 (near 1:3). This flip in ratios between the HasDiabetes groups suggests that there is a relationship between HasDiabetes and HighBP where a PT with high blood pressure will also have diabetes.\nThe next variable that we will investigate is HighCholesterol. Review of the variable summary information shows\n\nprint(list(\"Summary of HighCholesterol\"=summary(diabetes_df$HighCholesterol)))\n\n$`Summary of HighCholesterol`\n     0      1 \n144867 106611 \n\n\nthere is approximately a 14:11 “No” to “Yes” ratio of HighCholesterol observations. This distribution is fairly close to being 1:1, so any differences between the HasDiabetes categories would be due to interactions between the variables.\nUsing a bar plot to observe HighCholesterol with respect to HasDiabetes we see\n\nHighCholesterol_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, HighCholesterol) |&gt;\n  summarize(count = n())\n\nHighCholesterol_plot &lt;- ggplot(HighCholesterol_by, aes(HasDiabetes, count, fill=HighCholesterol)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of HighCholesterol by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nHighCholesterol_plot\n\n\n\n\n\n\n\n\nthat the ratios change between the HasDiabetes groups. While the ratios favor low cholesterol in the group without diabetes, they favor high cholesterol levels in the group with diabetes. There is approximately a 13:8 (nears 2:1) ratio of “No” to “Yes” HighCholesterol responses in the “No” HasDiabetes group. There is approximately a 11:23 (nears 1:2) ratio of “No” to “Yes” HighCholesterol responses in the “Yes” HasDiabetes group. The shift in ratios between the groups trending towards PTs with diabetes having high cholesterol suggests that there is a relationship between HasDiabetes and HighCholesterol where someone with high cholesterol will also have diabetes.\nThe next variable we will investigate is CholesterolChecked. Review of the variable summary information shows\n\nprint(list(\"Summary of CholesterolChecked\"=summary(diabetes_df$CholesterolChecked)))\n\n$`Summary of CholesterolChecked`\n     0      1 \n  9404 242074 \n\n\nthat the overwhelming majority of PTs had their cholesterol checked at a ratio of 9:242 (nears 1:27) when compared to those who did not. Any differences between the HasDiabetes groups may be heavily influenced by the number of observations in each group more than the interaction between the variables. Review of a bar plot of CholesterolChecked with respect to HasDiabetes shows\n\nCholesterolChecked_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, CholesterolChecked) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nCholesterolChecked_plot &lt;- ggplot(CholesterolChecked_by, aes(HasDiabetes, count, fill=CholesterolChecked)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of CholesterolChecked by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\")\n\nCholesterolChecked_plot\n\n\n\n\n\n\n\n\nthat the group of “Yes” CholesterolChecked responses dominates both categories of HasDiabetes. The “No” HasDiabetes group has a “No” to “Yes” CholesterolChecked ratio of approximately 9:208 (nears 1:23). The “Yes” HasDiabetes group has a “No” to “Yes” CholesterolChecked ratio of approximately 1:172. This information suggests that there is a relationship between the variables where someone with diabetes has a high probability of having their cholesteroal checked every five years, which is not what we’re trying to predict. CholesterolChecked does not appear to be a good variable for use in prediction modeling.\nThe next variable we’re going to analyze is Smoker. Review of the variable summary shows that\n\nprint(list(\"Summary of Smoker\"=summary(diabetes_df$Smoker)))\n\n$`Summary of Smoker`\n     0      1 \n139967 111511 \n\n\nthe ratio of smokers to non-smokers in the study is approximately 14:11. This is distribution between the groups is fairly close to a 1:1 distribution. Any differences between the HasDiabetes groups will be due to interactions between the two variables. Reviewing the bar plot of Smoker with respect to HasDiabetes shows\n\nSmoker_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Smoker) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nSmoker_plot &lt;- ggplot(Smoker_by, aes(HasDiabetes, count, fill=Smoker)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Smoker by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nSmoker_plot\n\n\n\n\n\n\n\n\nshows that the “No” HasDiabetes group has a “No” to “Yes” Smoker distribution of approximately 62:47 which is closer to a 1:1 distribution than a 2:1 distribution. The “Yes” HasDiabetes group has a “No” to “Yes” Smoker distribution of approximately 17:18 which also nears a 1:1 distribution. While the ratios shifts indicate that smokers have a higher likelihood of having diabetes than non smokers, neither Smoker group separates itself from the variable in both HasDiabetes categories. Smoker is not a suitable variable for inclusion in our prediction model.\nThe next variable we are going to analyze is Stroke. Review of the variable summary shows\n\nprint(list(\"Summary of Stroke\"=summary(diabetes_df$Stroke)))\n\n$`Summary of Stroke`\n     0      1 \n241295  10183 \n\n\nthat Stroke has a 24:1 “No” to “Yes” ratio. This suggests that any differences between the HasDiabetes groups may be heavily influenced by the number of observations in each group more than the interaction between the variables. Review of a bar plot for Stroke with respect to HasDiabetes shows\n\nStroke_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Stroke) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nStroke_plot &lt;- ggplot(Stroke_by, aes(HasDiabetes, count, fill=Stroke)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Stroke by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nStroke_plot\n\n\n\n\n\n\n\n\nthat the Stroke “No” group is prominent in both groups of HasDiabetes. The “No” HasDiabetes group has a “No to”Yes” Stroke ratio of approximately 211:7 which is close to 30:1. The “Yes” HasDiabetes group has a “No to”Yes” Stroke ratio of approximately 32:3 which is close to 11:1. While still favoring the “No” Stroke group, the reduction in ratios indicates that there is a relationship between Stroke and HasDiabetes. However, the relationship appears to be where someone who had a stroke is likely to also have diabetes, but not someone who has diabetes is likely to have had a stroke. Stroke does not appear to be suitable for inclusion in the prediction model.\nThe next variable we are going to analyze is HeartDisesaseorAttack. Summary information shows\n\nprint(list(\"Summary of HeartDiseaseorAttack\"=summary(diabetes_df$HeartDiseaseorAttack)))\n\n$`Summary of HeartDiseaseorAttack`\n     0      1 \n227838  23640 \n\n\nthat the ratio between the “No” and “Yes” HeartDiseaseorAttack groups is approximately 23:2 which is slightly larger than 11:1. Review of a bar plot for HeartDiseaseorAttack with respect to HasDiabetes shows\n\nHeartDiseaseorAttack_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, HeartDiseaseorAttack) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nHeartDiseaseorAttack_plot &lt;- ggplot(HeartDiseaseorAttack_by, aes(HasDiabetes, count, fill=HeartDiseaseorAttack)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of HeartDiseaseorAttack by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nHeartDiseaseorAttack_plot\n\n\n\n\n\n\n\n\nshows that the “No” HeartDiseaseorAttack group is noticably larger than the “Yes” group for each category of HasDiabetes. The “No” HeartDisease group has a “No” to “Yes” HeartDiseaseorAttack ratio of approximately 10:1. The “Yes” HeartDisease group has a “No” to “Yes” HeartDiseaseorAttack ratio of 27:7 which nears 4:1. The ratio change between the HasDiabetes groups suggests that there is a relationship between the variables. However, this relationship appears to be one where someone having diabetes can be used to predict if they have heart disease or have experienced a heart attack, which is not what we’re trying to predict. HeartDiseaseorAttack does not appear to be suitable for inclusion in the prediction model.\nThe next variable we will summarize is PhysActivity. Review of the variable summary information shows\n\nprint(list(\"Summary of PhysicalActivity\"= summary(diabetes_df$PhysActivity)))\n\n$`Summary of PhysicalActivity`\n     0      1 \n 60722 190756 \n\n\nthe ratio of “No” to “Yes” responses is approximately 6:19 which is slightly less than 1:3. This suggests that differences between the HasDiabetes groups may be influenced by the number of “Yes” PhysActivity observations. Review of a bar plot of PhysActivity with respect to HasDiabetes shows\n\nPhysActivity_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, PhysActivity) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nPhysActivity_plot &lt;- ggplot(PhysActivity_by, aes(HasDiabetes, count, fill=PhysActivity)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of PhysActivity by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nPhysActivity_plot\n\n\n\n\n\n\n\n\nPhysActivity is greater in both HasDiabetes groups. The “No” HeartDisease group has a “No” to “Yes” PhysActivity ratio of approximately 5:17 which is slightly less than 1:3. The “Yes” HeartDisease group has a “No” to “Yes” PhysActivity ratio of approximately 13:22 which is slightly greater than 1:2. This change in ratios shows that there is a relationship between the variables. The relationship appears to be one where someone having diabetes can be used to predict if they participate in physical activity or not, which is not what we’re trying to predict. PhysActivity does not appear to be suitable for inclusion in the prediction model.\nThe next variable we’re going to summarize is ConsumesFruits. Review of the variable summary information shows\n\nprint(list(\"Summary of ConsumesFruits\"=summary(diabetes_df$ConsumesFruits)))\n\n$`Summary of ConsumesFruits`\n     0      1 \n 91734 159744 \n\n\nthat ConsumesFruits has a “No” to “Yes” ratio of approximately 9:16, which is slightly greater than a 1:2 distribution. Review of a bar graph of ConsumesFruits with respect to HasDiabetes shows\n\nConsumesFruits_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, ConsumesFruits) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nConsumesFruits_plot &lt;- ggplot(ConsumesFruits_by, aes(HasDiabetes, count, fill=ConsumesFruits)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of ConsumesFruits by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nConsumesFruits_plot\n\n\n\n\n\n\n\n\nthat both groups of HasDiabetes have a greater number of “Yes” ConsumesFruits observations. The “No” HasDiabetes group has a “No” to “Yes” ConsumesFruits ratio of approximately 8:14 which is slightly greater than 1:2. The “Yes” HasDiabetes group has a “No” to “Yes” ConsumesFruits ratio of approximately 15:21 which is slightly greater than 2:3. This change in ratios shows that there is a relationship between the variables. However, the relationship appears to be one where someone having diabetes can be used to predict if they eat fruits or not, which is not what we’re tyring to predict. ConsumesFruits does not appear to be suitable for inclusion in the prediction model.\nThe next variabel we’re going to summarize is **ConsumesVeggies*. Review of the variable summary information shows\n\nprint(list(\"Summary of ConsumesVeggies\"=summary(diabetes_df$ConsumesVeggies)))\n\n$`Summary of ConsumesVeggies`\n     0      1 \n 47284 204194 \n\n\nthat the proporiton of “No” to “Yes” responses is approximately 5:21 which is slightly less than 1:4. Review of a bar plot of ConsumesVeggies with respect to HasDiabetes shows\n\nConsumesVeggies_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, ConsumesVeggies) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nConsumesVeggies_plot &lt;- ggplot(ConsumesVeggies_by, aes(HasDiabetes, count, fill=ConsumesVeggies)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of ConsumesVeggies by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nConsumesVeggies_plot\n\n\n\n\n\n\n\n\nthat the “Yes” ConsumesVeggies observations are substantially greater in both categories of HasDiabetes. The “No” HasDiabetes group has a “No” to “Yes” ConsumesVeggies ratio of approximately 2:9 which is slightly less than 1:4. The “Yes” HasDiabetes group has a “No” to “Yes” ConsumesVeggies ratio of approximately 1:3. The change in ratios between the HasDiabetes groups shows that there is a relationship between the variables. However, the relationship appears to be one where someone having diabetes can be used to predict if they eat vegetables or not, which is not what we’re trying to predict. ConsumesVeggies does not appear to be suitable for inclusion in the prediction model.\nThe next variable we’re going to summarize is HeavyAlcoholUse. Review of the variable summary information shows\n\nprint(list(\"Summary of HeavyAlcoholUse\"=summary(diabetes_df$HeavyAlcoholUse)))\n\n$`Summary of HeavyAlcoholUse`\n     0      1 \n237283  14195 \n\n\nshows that the ratio of “No” to “Yes” responses is approximately 24:1. This suggests that any differences between the HasDiabetes groups may be influenced by the number of observations. Reviewing the bar plot of HeavyAlcoholUse with respect to **HeartDisease* shows\n\nHeavyAlcoholUse_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, HeavyAlcoholUse) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nHeavyAlcoholUse_plot &lt;- ggplot(HeavyAlcoholUse_by, aes(HasDiabetes, count, fill=HeavyAlcoholUse)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of HeavyAlcoholUse by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nHeavyAlcoholUse_plot\n\n\n\n\n\n\n\n\nthat the “No” HeavyAlcoholUse responses constitute the majority of observations in both HasDiabetes groups. The “No” HasDiabetes group has a “No” to “Yes” HeavyAlcoholUse ratio of approximately 205:13 which reduces close to 16:1. The “Yes” HasDiabetes group has a “No” to “Yes” HeavyAlcoholUse ratio of approximately 345:8 which reduces close to 43:1. The change in ratios shows that there is a relationship between the variables. However, the relationship appears to be one where someone having diabetes can be used to predict if they consume alcohol heavily, which is not what we’re trying to predict. HeavyAlcoholUse does not appear to be suitable for inclusion in the prediction model.\nThe next variable we’re going to summarize is HasHealthcare. Review of the variable summary information\n\nprint(list(\"Summary of HasHealthcare\"=summary(diabetes_df$HasHealthcare)))\n\n$`Summary of HasHealthcare`\n     0      1 \n 12246 239232 \n\n\nshows that the ratio of “No” to “Yes” observations is approximately 12:241 which is close to a 1:20 ratio. This is suggestive that any differences between the HasDiabetes categories may be heavily influenced by the number of responses in each. Reviewing the bar chart of HasHealthcare with respect to HasDiabetes shows\n\nHasHealthcare_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, HasHealthcare) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nHasHealthcare_plot &lt;- ggplot(HasHealthcare_by, aes(HasDiabetes, count, fill=HasHealthcare)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of HasHealthcare by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nHasHealthcare_plot\n\n\n\n\n\n\n\n\nthe “Yes” HasHealthcare observations are substantially higher than the “No” responses for both categories of **HasDiabetes*. The “No” HasDiabetes group has a “No” to “Yes” HasHealthcare ratio of approximately 11:207 which reduces close to 1:19. The “Yes” HasDiabetes group has a “No” to “Yes” HasHealthcare ratio of approximately 14:339 which reduces close to 1:24. The ratios in both groups of HasDiabetes do not differ much from the overall HasHealthcare variable ratio of 1:20. HasHealthcare does not appear to be suitable for inclusion in the prediction model.\nThe next variable we’re going to summarize is ExpensiveTreatment. Review of the variable summary information\n\nprint(list(\"Summary of ExpensiveTreatment\"=summary(diabetes_df$ExpensiveTreatment)))\n\n$`Summary of ExpensiveTreatment`\n     0      1 \n230457  21021 \n\n\nthat the ration of “No” to “Yes” observations is approximately 232:21 which reduces near 11:1. This is suggestive that any differences between the HasDiabetes categories may be heavily influenced by the number of repsonses in each. Reviwing the bar chart of ExpensiveTreatment with respect to HasDiabetes shows\n\nExpensiveTreatment_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, ExpensiveTreatment) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nExpensiveTreatment_plot &lt;- ggplot(ExpensiveTreatment_by, aes(HasDiabetes, count, fill=ExpensiveTreatment)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of ExpensiveTreatment by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nExpensiveTreatment_plot\n\n\n\n\n\n\n\n\nthe “No” observations are substantially greater than the “Yes” responses for both categories of HasDiabetes. The “No” HasDiabetes group has a “No” to “Yes” ExpensiveTreatment ratio of approximately 201:17 which reduces close to 12:1. The “Yes” HasDiabetes group has a “No” to “Yes” ExpensiveTreatment ratio of approximately 316:37 which reduces close to 9:1. There appears to be a relationship between the variables where someone with diabetes has a greater likelihood of not seeking treatment due to the cost, but that is not what we’re trying to predict. ExpensiveTreatment does not appear to be suitbale for inclusion in the prediciton model.\nThe next variable we’re going to summarize is GeneralHealth. Review of the variable summary information\n\nprint(list(\"Summary of GeneralHealth\"=summary(diabetes_df$GeneralHealth)))\n\n$`Summary of GeneralHealth`\n    1     2     3     4     5 \n45159 88704 74967 30900 11748 \n\n\nshows that the majority of patients reported being in “Excellent” to “Good” health. Reviewing the bar chart of GeneralHealth with respect to HasDiabetes shows\n\nGeneralHealth_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, GeneralHealth) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nGeneralHealth_plot &lt;- ggplot(GeneralHealth_by, aes(HasDiabetes, count, fill=GeneralHealth)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of GeneralHealth by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nGeneralHealth_plot\n\n\n\n\n\n\n\n\nthat there is a shift in the reporting tendencies between the HasDiabetes categories. The “No” HasDiabetes group has a central tendency of “Very Good” followed by “Good” GeneralHealth responses while the “Yes” HasDiabetes group has a central tendency of “Good” followed by “Fair” GeneralHealth responses\n\nNoDiabetes_high_three &lt;- sum(GeneralHealth_by$count[1:3])/sum(GeneralHealth_by$count[1:5])\nNoDiabetes_fourth &lt;- GeneralHealth_by$count[4]/sum(GeneralHealth_by$count[1:5])\n\nYesDiabetes_high_three &lt;- sum(GeneralHealth_by$count[7:9])/sum(GeneralHealth_by$count[6:10]) \nYesDiabetes_fourth &lt;- GeneralHealth_by$count[10]/sum(GeneralHealth_by$count[6:10])\n\nprint(list(\"GeneralHealth 1 through 3, HasDiabetes in 'No'\"=NoDiabetes_high_three, \"GeneralHealth 4, HasDiabetes in 'No'\"=NoDiabetes_fourth, \"GeneralHealth 2 through 4, HasDiabetes in 'Yes'\"=YesDiabetes_high_three, \"GeneralHealth 5, HasDiabetes in 'Yes'\"=NoDiabetes_fourth))\n\n$`GeneralHealth 1 through 3, HasDiabetes in 'No'`\n[1] 0.8674027\n\n$`GeneralHealth 4, HasDiabetes in 'No'`\n[1] 0.09865936\n\n$`GeneralHealth 2 through 4, HasDiabetes in 'Yes'`\n[1] 0.8406513\n\n$`GeneralHealth 5, HasDiabetes in 'Yes'`\n[1] 0.09865936\n\n\nReview of the GeneralHealth response proportions in the “No” HasDiabetes group shows that over 86.59% of the observations reported having a GeneralHealth status of “Good” or better for the three highest responses with the 4th highest response of “Fair” accounting for another 9.98% of group responses. Review of the GeneralHealth response proportions in the “Yes” HasDiabetes shows that over 83.82% of the observations reported having a GeneralHealth status of “Very Good” to “Fair” for the three highest responses with the fourth highest response of “Poor” accounting for another 9.98% of group responses. The shift in GeneralHealth tendency between the HasDiabetes groups suggests that there is a relationship between the variables. GeneralHealth appears to be a suitable variable for prediciton modeling.\nThe next variable we’re going to summarize is BadMentalHealth. Review of the variable summary information\n\nprint(list(\"Summary of BadMentalHealth\"=summary(diabetes_df$BadMentalHealth)))\n\n$`Summary of BadMentalHealth`\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   3.155   2.000  30.000 \n\n\nshows that there is a tendency for the PTs to report that their mental health was not good for zero days.\nReview of a histogram of the variable along with histograms of MentalHealth with respect to HasDiabetes shows\n\nBadMentalHealth_histogram &lt;- ggplot(diabetes_df, aes(BadMentalHealth)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  scale_x_continuous(breaks = seq(0, 30, by = 5)) +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.05)) +\n  labs(title=\"Histogram of BadMentalHealth\", x=\"# of Days Mental Health Not Good\", y=\"Density (# of PTs)\")\n\nBadMentalHealth_histogram\n\n\n\n\n\n\n\nBadMentalHealth_by_histplot &lt;- ggplot(diabetes_df, aes(BadMentalHealth)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  labs(title=\"Histogram of BadMentalHealth by HasDiabetes\", y=\"Density (# of PTs)\") +\n  scale_x_continuous(breaks = seq(0, 30, by = 5)) +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.05)) +\n  facet_grid(~HasDiabetes)\n\nBadMentalHealth_by_histplot\n\n\n\n\n\n\n\n\nthat the majority of the observations in both categories of HasDiabetes follow the similar trend where the majority of non-zero values in BadMentalHealth are in the range of 1-10 days. After remvoing the zero values for analysis, we compared the proportions of 10-day BadMentalHealth ranges between the HasDiabetes groups.\n\nBadMentalHealth_by_nozero &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, BadMentalHealth) |&gt;\n  filter(BadMentalHealth &gt;= 1) |&gt;\n  summarize(count=n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nprint(BadMentalHealth_by_nozero, n=60)\n\n# A tibble: 60 × 3\n# Groups:   HasDiabetes [2]\n   HasDiabetes BadMentalHealth count\n   &lt;fct&gt;                 &lt;dbl&gt; &lt;int&gt;\n 1 0                         1  7679\n 2 0                         2 11465\n 3 0                         3  6403\n 4 0                         4  3259\n 5 0                         5  7729\n 6 0                         6   819\n 7 0                         7  2672\n 8 0                         8   524\n 9 0                         9    78\n10 0                        10  5256\n11 0                        11    38\n12 0                        12   330\n13 0                        13    32\n14 0                        14   957\n15 0                        15  4422\n16 0                        16    73\n17 0                        17    42\n18 0                        18    76\n19 0                        19    12\n20 0                        20  2654\n21 0                        21   178\n22 0                        22    50\n23 0                        23    30\n24 0                        24    27\n25 0                        25   896\n26 0                        26    37\n27 0                        27    66\n28 0                        28   268\n29 0                        29   128\n30 0                        30  9183\n31 1                         1   788\n32 1                         2  1481\n33 1                         3   900\n34 1                         4   476\n35 1                         5  1186\n36 1                         6   158\n37 1                         7   391\n38 1                         8   106\n39 1                         9    12\n40 1                        10  1033\n41 1                        11     3\n42 1                        12    62\n43 1                        13     8\n44 1                        14   190\n45 1                        15   989\n46 1                        16    12\n47 1                        17    11\n48 1                        18    19\n49 1                        19     4\n50 1                        20   639\n51 1                        21    46\n52 1                        22    10\n53 1                        23     8\n54 1                        24     6\n55 1                        25   258\n56 1                        26     6\n57 1                        27    11\n58 1                        28    54\n59 1                        29    27\n60 1                        30  2664\n\nHasDiabetesNo_first_ten &lt;- list(\"HasDiabetesNo, BadMentalHealth, 1-10 days proprotion of data\" =sum(BadMentalHealth_by_nozero$count[1:10])/sum(BadMentalHealth_by_nozero$count[1:30]))\n\nHasDiabetesNo_middle_ten &lt;- list(\"HasDiabetesNo, BadMentalHealth, 11-20 days proprotion of data\" =sum(BadMentalHealth_by_nozero$count[11:20])/sum(BadMentalHealth_by_nozero$count[1:30]))\n\nHasDiabetesNo_last_ten &lt;- list(\"HasDiabetesNo, BadMentalHealth, 21-30 days proprotion of data\" =sum(BadMentalHealth_by_nozero$count[21:30])/sum(BadMentalHealth_by_nozero$count[1:30]))\n\nHasDiabetesYes_first_ten &lt;- list(\"HasDiabetesYes, BadMentalHealth, 1-10 days proprotion of data\"=sum(BadMentalHealth_by_nozero$count[31:40])/sum(BadMentalHealth_by_nozero$count[31:60]))\n\nHasDiabetesYes_middle_ten &lt;- list(\"HasDiabetesYes, BadMentalHealth, 11-20 days proprotion of data\"=sum(BadMentalHealth_by_nozero$count[41:50])/sum(BadMentalHealth_by_nozero$count[31:60]))\n\nHasDiabetesYes_last_ten &lt;- list(\"HasDiabetesYes, BadMentalHealth, 20-30 days proprotion of data\"=sum(BadMentalHealth_by_nozero$count[51:60])/sum(BadMentalHealth_by_nozero$count[31:60]))\n\nprint(c(HasDiabetesNo_first_ten, HasDiabetesYes_first_ten, HasDiabetesNo_middle_ten, HasDiabetesYes_middle_ten, HasDiabetesNo_last_ten, HasDiabetesYes_last_ten))\n\n$`HasDiabetesNo, BadMentalHealth, 1-10 days proprotion of data`\n[1] 0.7017726\n\n$`HasDiabetesYes, BadMentalHealth, 1-10 days proprotion of data`\n[1] 0.5650632\n\n$`HasDiabetesNo, BadMentalHealth, 11-20 days proprotion of data`\n[1] 0.1320833\n\n$`HasDiabetesYes, BadMentalHealth, 11-20 days proprotion of data`\n[1] 0.1675895\n\n$`HasDiabetesNo, BadMentalHealth, 21-30 days proprotion of data`\n[1] 0.1661441\n\n$`HasDiabetesYes, BadMentalHealth, 20-30 days proprotion of data`\n[1] 0.2673473\n\n\nThe comparisons showed that the “No” HasDiabetes group had a 70% of their BadMentalHealth responses in the 1-10 day range while the “Yes” HasDiabetes group only had a 56.2% of their BadMentalHealth responses in the same range. Likewise, the “Yes” HasDiabetes group had higher percentages of responses in both the 11-20 day and 21-30 day BadMentalHealth ranges than the “No” HasDiabetes group. The change in “0” response density between the HasDiabetes groups appears to have shifted to longer periods of PTs with diabetes experiencing mental health that is not good. This shows that there is a relationship between the variables. BadMentalHealth appears to be suitable for prediction modeling.\nThe next variable we’re going to summarize is BadPhysicalHealth. Review of the variable summary information\n\nprint(list(\"Summary of PhysicalHealth\"=summary(diabetes_df$BadPhysicalHealth)))\n\n$`Summary of PhysicalHealth`\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   4.194   3.000  30.000 \n\n\nshows that like BadMentalHealth, the vast majority of BadPhysicalHealth responses provided were “0”. Reviewing a density plot histogram of BadPhysicalHealth along with histogram plots of BadPhysicalHealth with respect to HasDiabetes show\n\nBadPhysicalHealth_histogram &lt;- ggplot(diabetes_df, aes(BadPhysicalHealth)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  scale_x_continuous(breaks = seq(0, 30, by = 5)) +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.05)) +\n  labs(title=\"Histogram of BadPhysicalHealth\", x=\"# of Days Mental Health Not Good\", y=\"Density (# of PTs)\")\n\nBadPhysicalHealth_histogram\n\n\n\n\n\n\n\nBadPhysicalHealth_by_histplot &lt;- ggplot(diabetes_df, aes(BadPhysicalHealth)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  labs(title=\"Histogram of BadPhysicalHealth by HasDiabetes\", y=\"Density (# of PTs)\") +\n  scale_x_continuous(breaks = seq(0, 30, by = 5)) +\n  scale_y_continuous(breaks = seq(0, 30, by = 0.05)) +\n  facet_grid(~HasDiabetes)\n\nBadPhysicalHealth_by_histplot\n\n\n\n\n\n\n\n\nthat like with BadMentalHealth, the density of the BadPhysicalHealth “0” response changes between the HasDiabetes groups. The general trend is that when compared to the “No” HasDiabetes group the “Yes” HasDiabetes densities in the 0-7 day range are lower and then overall higher among the remaining groups. Comparing the proportions of the 0-7day ranges to the 8-30 day ranges\n\nBadPhysicalHealth_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, BadMentalHealth) |&gt;\n  summarize(count=n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nBadPhysHlth_DiabetesNo_firstSeven &lt;- list(\"BadPhysHealth 1-7 Days, Diabetes in 'No'\" = sum(BadPhysicalHealth_by$count[1:8])/sum(BadPhysicalHealth_by$count[1:31]))\n\nBadPhysHlth_DiabetesNo_lastTwentyThree &lt;- list(\"BadPhysHealth 8-30 Days, Diabetes in 'No'\" = sum(BadPhysicalHealth_by$count[9:31])/sum(BadPhysicalHealth_by$count[1:31]))\n\nBadPhysHlth_DiabetesYes_firstSeven &lt;- list(\"BadPhysHealth 1-7 Days, Diabetes in 'Yes'\" = sum(BadPhysicalHealth_by$count[32:39])/sum(BadPhysicalHealth_by$count[32:62]))\n\nBadPhysHlth_DiabetesYes_lastTwentyThree &lt;- list(\"BadPhysHealth 8-30 Days, Diabetes in 'Yes'\" = sum(BadPhysicalHealth_by$count[40:62])/sum(BadPhysicalHealth_by$count[32:62]))\n\nprint(c(BadPhysHlth_DiabetesNo_firstSeven, BadPhysHlth_DiabetesYes_firstSeven, BadPhysHlth_DiabetesNo_lastTwentyThree, BadPhysHlth_DiabetesYes_lastTwentyThree))\n\n$`BadPhysHealth 1-7 Days, Diabetes in 'No'`\n[1] 0.8830596\n\n$`BadPhysHealth 1-7 Days, Diabetes in 'Yes'`\n[1] 0.8216564\n\n$`BadPhysHealth 8-30 Days, Diabetes in 'No'`\n[1] 0.1169404\n\n$`BadPhysHealth 8-30 Days, Diabetes in 'Yes'`\n[1] 0.1783436\n\n\nconfirms that the visual assessments are correct. The “No” HasDiabetes group is 6.4 percentage points higher in the BadPhysicalHealth 0-7 day range compared to the “Yes” HasDiabetes group. This change in HasDiabetes suggests that there is a relationship between the variables. **BadPhysicalHealth* appears to be suitable for prediction modeling.\nThe next variable we’re going to summarize is DifficultyWalking. Reviewing the variable summary\n\nprint(list(\"Summary of DifficultyWalking\"=summary(diabetes_df$DifficultyWalking)))\n\n$`Summary of DifficultyWalking`\n     0      1 \n209904  41574 \n\n\nshows that the majority of PTs reported “No” at a ratio of approximately 211:43, which reduces to slightly less than 5:1. This suggests that any differences in the HasDiabetes groups will be highly influenced by the number of PTs in each. Reviewing the bar plot of DifficultyWalking with respect to HasDiabetes shows\n\nDifficultyWalking_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, DifficultyWalking) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nDifficultyWalking_plot &lt;- ggplot(DifficultyWalking_by, aes(HasDiabetes, count, fill=DifficultyWalking)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of DifficultyWalking by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nDifficultyWalking_plot\n\n\n\n\n\n\n\n\nthat the majority of responses in both HasDiabetes groups fall under the “No” DifficultyWalking group. The observations in the “No” HasDiabetes category have a “No” to “Yes” DifficultyWalking ratio of approximately 188:30 which simplifies down to a ratio slightly greater than 6:1. The observations in the “Yes” HasDiabetes category have a “No” to “Yes” DifficultyWalking ratio of approximately 222:131 which simplifies down to closer to 2:1 than 1:1. The change in ratios suggests that there is a relationship between the variabels. However, the relationship appears to be one where someone having diabetes or not can help predict if they’ll have difficulty walking, but that is not what we’re trying to predict. DifficultyWalking does not appear to be a suitable variable for our prediction model.\nThe next variable we’re going to summarize is Sex. Reviewing the variable summary\n\nprint(list(\"Summary of Sex\"=summary(diabetes_df$Sex)))\n\n$`Summary of Sex`\n     0      1 \n140542 110936 \n\n\nshows that the distribution between the categories is almost uniform at a female to male ratio of approximately 142:111 which simplifies to a ratio slightly greater than 1:1. Reviewing the bar plot of Sex with respect to HasDiabetes shows\n\nSex_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Sex) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nSex_plot &lt;- ggplot(Sex_by, aes(HasDiabetes, count, fill=Sex)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Sex by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nSex_plot\n\n\n\n\n\n\n\n\nthat the number of females is greater in both HasDiabetes groups. The observations in the “No” HasDiabetes category have a “No” to “Yes” Sex ratio of approximately 124:95 which simplifies down to a ratio of slightly greater than 1:1. The observations in the “Yes” HasDiabetes category have a “No” to “Yes” Sex ratio of approximately 184:169 which also simplifies down to a ratio slightly greater than 1:1. The difference between the groups is minimal which suggests there is not a relationship between the variables. Sex does not appear suitable for our prediciton model.\nThe next variable we’re going to summarize is Age. Review of the variable summary\n\nprint(list(\"Summary of Age\"=summary(diabetes_df$Age)))\n\n$`Summary of Age`\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n 5659  7506 10947 13680 15963 19569 26002 30503 32959 31963 23451 15942 17334 \n\n\nshows that there is a trend where the number of responses increase in each category until group 9 (60-64), where they then decrease as from category 10 to 12 with group 13 being slightly higher than group 12. Reviewing histograms of Age with respect to HasDiabetes shows\n\nAge_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Age) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nAge_plot &lt;- ggplot(Age_by, aes(HasDiabetes, count, fill=Age)) +\n  geom_bar(stat = \"identity\", position = position_dodge2()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25, size=3) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Age by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\")\n\nAge_plot\n\n\n\n\n\n\n\n\nthat there a shift in distribution between the HasDiabetes groups. The “No” HasDiabetes group has central tendency around resonse “9” while the “Yes” HasDiabetes group has a central tendency around response “10”. Review of a summary table of the proportions between the HasDiabetes categories by groups shows\n\nAgeGrp &lt;- Age_by$Age[1:13]\n\nproportionNo &lt;- proportions(Age_by$count[1:13])\n\nproporitonYes &lt;-proportions(Age_by$count[14:26])\n\nAge_Diabetes &lt;- data.frame(AgeGrp, proportionNo, proporitonYes)\n\nAge_Diabetes &lt;- as_tibble(Age_Diabetes)\n\nprint(list(\"Summary Table of Age Proportions\"=Age_Diabetes))\n\n$`Summary Table of Age Proportions`\n# A tibble: 13 × 3\n   AgeGrp proportionNo proporitonYes\n   &lt;fct&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 1            0.0257       0.00219\n 2 2            0.0340       0.00367\n 3 3            0.0491       0.00849\n 4 4            0.0603       0.0172 \n 5 5            0.0690       0.0288 \n 6 6            0.0825       0.0482 \n 7 7            0.106        0.0860 \n 8 8            0.122        0.120  \n 9 9            0.126        0.162  \n10 10           0.118        0.186  \n11 11           0.0846       0.147  \n12 12           0.0579       0.0979 \n13 13           0.0652       0.0923 \n\nproportionNo_largestThree &lt;- sum(Age_Diabetes$proportionNo[8:10])\n\nproportionYes_largestThree &lt;- sum(Age_Diabetes$proporitonYes[9:11])\n\nprint(list(\"Cummulative Proportion of 3 largest Age groups, HasDiabetes - No\"=proportionNo_largestThree, \"Cummulative Proportion of 3 largest Age groups, HasDiabetes - Yes\"=proportionYes_largestThree))\n\n$`Cummulative Proportion of 3 largest Age groups, HasDiabetes - No`\n[1] 0.3653251\n\n$`Cummulative Proportion of 3 largest Age groups, HasDiabetes - Yes`\n[1] 0.4956554\n\n\nthat not only did the distributions shift between the age groups, but the highest three Age categories (8-10) in the “No” HasDiabetes group account for 36.51% of the group data while the highest three Age categories (9-11) in the “Yes” HasDiabetes category account for 49.32% of the group data. These shifts in proportions suggest that there is a relationship between the variables. Age appears to be suitable for prediction modeling.\nThe next variabel we’re going to summarize is Education. Review of the variable summary information\n\nprint(list(\"Summary of Education\"=summary(diabetes_df$Education)))\n\n$`Summary of Education`\n     1      2      3      4      5      6 \n   170   3975   9342  62130  69218 106643 \n\n\nshows that the responses increased in each category with the majority of observations belonging to category 6 (College 4yrs or more (College graduate)). Reviewing the bar plot of Education with respect to HasDiabetes shows\n\nEducation_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Education) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nEducation_plot &lt;- ggplot(Education_by, aes(HasDiabetes, count, fill=Education)) +\n  geom_bar(stat = \"identity\", position = position_dodge2()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Education by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nEducation_plot\n\n\n\n\n\n\n\n\nshows that there is a definite change in the ratios of the Education gropus between the HasDiabetes groups. The “No” HasDiabetes group follows the same trend as the overall data while the “Yes” HasDiabetes group has a fairly uniform distribution between categories 4, 5, and 6. There does appear to be a relationship between the variables where we could determine there was a high probability that someone with diabetes has a high probability of having a high school diploma, GED, or higher education but the data does not lend itself to determining if a given education level can be used to predict if someone has diabetes. Education does not appear to be suitable for prediction analysis.\nThe last variable that we’re going to summarize is Income. Review of the variable summary information\n\nprint(list(\"Summary of Income\"=summary(diabetes_df$Income)))\n\n$`Summary of Income`\n    1     2     3     4     5     6     7     8 \n 9598 11542 15780 19882 25635 36184 42907 89950 \n\n\nshows that the number of responses increase as the categories increase, with the largest group earning $75K per year or more. Reviewing a bar plot of Income with respect to HasDiabetes shows\n\nIncome_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Income) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nIncome_plot &lt;- ggplot(Income_by, aes(HasDiabetes, count, fill=Income)) +\n  geom_bar(stat = \"identity\", position = position_dodge2()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25, size=3) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Income by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\")\n\nIncome_plot\n\n\n\n\n\n\n\n\nthat there is a shift in proportions of Income group categories between the HasDiabetes groups. The Income groups with “No” HasDiabetes responses appear to have a quadratic relationship with a large positive slope between groups 7 and 8. The Income groups with “Yes” HasDiabetes responses appear to have a linear relationship with a small positive slope. There appears to be a relationship between the variables where a person with diabetes has a greater probability of earning less than $75K pe year, but that is not what we’re trying to predict. Income does not appear to be suitable for prediction modeling.\nNow that we have identified the variables with relationships to HasDiabetes, we will consider if they are independent from one another. Review of a heat plot of HasDiabetes compared to the six predictor variables shows\n\ndiabetes_df_model &lt;- diabetes_df |&gt;\n  select(HasDiabetes, HighCholesterol, BMI, GeneralHealth, BadMentalHealth, BadPhysicalHealth, Age) \n\nindependenceCheck &lt;- model.matrix(~., \n             data=diabetes_df_model) |&gt;\n  cor(use=\"pairwise.complete.obs\") |&gt;\n  ggcorrplot(title=\"Heat Plot of Predictor Variable Correlations\", show.diag=FALSE, type=\"lower\", lab=TRUE, lab_size=1.5) +\n  theme(axis.text.x = element_text(size=6),\n        axis.text.y = element_text(size=6))\n\nprint(independenceCheck)\n\n\n\n\n\n\n\n\nthere are notable interactions between GeneralHealth, BadMentalHealth, and BadPhysicalHealth.\nAs GeneralHealth approaches category 5, the value of the correlation coefficient between it and BadPhysicalHealth increases to a moderate degree of correlation with maximum value of 0.49. GeneralHealth categories 2 through 4 all have weak correlation coefficients with BadPhysicalHealth ranging from -0.23 to 0.31.\nAs GeneralHealth approaches category 5, the value of the correlation coefficient between it and BadMentalHealth increases to a weak degree of correlation with maximum value of 0.26. GeneralHealth categories 2 through 4 also have weak correlation coefficients with BadMentalHealth ranging from -0.13 to 0.17.\nBadMentalHealth and BadPhysicalHealth have a moderate degree of correlation with a correlation coefficient of 0.35.\nBecause BadPhysicalHealth greatly influences both GeneralHealth and BadMentalHealth, because BadMentalHealth also influences GeneralHealth, and because BadPhysicalHealth has the highest correlation coefficient with HasDiabetes, it would be reasonable to remove GeneralHealth and BadMentalHealth from being used as predictor variables due to their dependence on BadPhysicalHealth.\nThe remaining combinations of variable groups all have weak correlation coefficients in the -0.30 to 0.30 range. Each of these variables appear to be independent from one another.\nNow that we have completed our EDA, we will use the above information in our Final Project Modeling"
  },
  {
    "objectID": "FinalProjectEDA.html#diabetes-health-indicators---eda",
    "href": "FinalProjectEDA.html#diabetes-health-indicators---eda",
    "title": "Final Project",
    "section": "",
    "text": "The diabetes binary health indicators file is a subset of 22 variables and 253,690 responses to questions asked of as part of the Behavioral Risk Factor Surveillance System (BHFSS) survey conducted in 2015.\nThe variables on the binary health indicators file are:\n\nHasDiabetes is the target variable that we want to predict. It is a categorical variable with levels “0” for “No diabetes”, “1” for “Pre-diabetes”, and “2”” for “Diabetes”“. This variable has been renamed from the initial name of Diabetes_binary.\nHighBP is a categorical variable with levels “0” for “No high BP” and “1” for “High BP”.\nHighCholesterol is a categorical variable with levels “0” for “No High Cholesterol” and “1” for “High Cholesterol”. This variable has been renamed from the initial name of HighChol.\nCholesterolChecked is a categorical variable with levels “0” for “No cholesterol check in 5 years” and “1” for “Cholesterol check in 5 years”. This variable has been renamed from the initial name of CholCheck.\nBMI is a discrete numeric variable indicating the BMI level of the PT observed.\nSmoker is a factor variable with levels “0” for “No” and “Yes” as responses to the question “Have you smoked at least 100 cigarettes in your life?”.\nStroke is a categorical variable with levels “0” for “No” and “1” for “Yes” as reponses to the question “Have you ever had or been told you had a stroke?”.\nHeartDiseaseorAttack is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Does the PT have coronary heart disease (CHD) or myocardial infraction (MI)?”.\nPhysActivity is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Not including your job, have you participated in physical activity in the past 30 days?”.\nConsumesFruits is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Do you consume fruit 1 or more times per day?”. This variable has been renamed from the initial name of Fruits.\nConsumesVeggies is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Do you consume vegetables 1 or more times per day?”. This variable has been renamed from the initial name of Veggies.\nHeavyAlcoholUse is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “How many drinks do you consume each week?”. Men responding to the question were placed in the “Yes” category if they consumed 14 or more alcoholic drinks in a week, otherwise they were placed in the “No” category. Women responding to the question were placed in the “Yes” category if they consumed 7 or more alcoholic drinks in a week, otherwise they were placed in the “No” category. This variable has been renamed from the initial name of HvyAlcoholConsump.\nHasHealthcare is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Do you have any kind of health care coverage?”. Responses stating the individual had health insurance or prepaid plans were grouped into the “Yes” category. This variable has been renamed from the initial name of AnyHealthcare.\nExpensiveTreatment is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?”. This variable has been renamed from the initial name of NoDocbcCost.\nGeneralHealth is a categorical variable with response levels “1” for “Excellent”, “2” for “Very Good”, “3 for”Good”, “4” for “Fair”, and “5” for “Poor” to the question “Would you say your general health is…?”. This variable has been renamed from the initial name of GenHlth.\nBadMentalHealth is a categorical variable with levels “0” through “30” as responses to the question “Over the last 30 days, how many did you experience poor mental health”. Given the thirty different possible responses, it may be suitable to treat BadMentalHealth as a numeric variable. This variable has been renamed from the initial name of MentHlth.\nBadPhysicalHealth is a categorical variable with levels “0” through “30” as responses to the question “Over the last 30 days, how many did you experience having physical illness or an injury?”. Given the thirty different possible responses, it may be suitable to treat BadPhysicalHealth as a numeric variable. This variable has been renamed from the initial name of PhysHlth.\nDifficultyWalking is a categorical variable with levels “0” for “No” and “1” for “Yes” to the question “Do you have serious difficulty walking or climbing stairs?”.\nSex is a categorical variable with levels “0” for “Female” and “1” for “Male” to the question “What is your Sex?”.\nAge is a categorical variable with levels ranging from “1” for “18-24” to “13” for “80 or Older” to the question “What is your age?”. The response were binned together in 5year groups with the exception of groups 1 and 13. Group 1 was expanded to account for PTs that were 18yo or 19yo. Group 13 was created to account for anyone who was 80yo or older.\nEducation is a categorical variable with levels “1” through “8” as responses to the question “What is the highest grade or year of school you completed?”. The response levels are “1” for “Never attended school or only Kindergarten”, “2” for “Grades 1 through 8 (Elementary)”, “3” for “Grades 9 through 11 (Some high school)”, “4” for “Grade 12 or GED (High school graduate)”, “5” for “College 1 year to 3 years (Some college or technical school)”, and “6” for “College 4 years or more (College graduate)”.\nIncome is a categorical variable with levels “1” through “8” as reponses to the question “Is your annual income from all sources:?”. The response levels are “1” for “Less than $10,000”, “2” for “Less than $15,000”, “3” for “Less than $20,000”, “4” for “Less than $25,000”, “5” for “Less than $35,000”, “6” for “Less than $50,000”, “7” for “Less than $75,000”, and “8” for “$75,000 or more”.\n\n\n\n\nThe purpose of the Exploratory Data Analysis (EDA) is to summarize the information from the twenty-one predictor variables with respect to Diabetes_binary and determine if any relationships exist.\nThe goal of the EDA is to identify the variables having relationships with Diabetes_binary that are also suitable for prediction modeling. We will use these variables to generate different prediction model types and determine one that has the highest rate of success at predicting if someone has diabetes or not.\n\n\n\n\nBefore we read-in the data set, we need to establish a library required to use the functions required for summary analysis in R.\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(GGally)\nlibrary(ggcorrplot)\n\nNext, we will read-in the data using a relative path to the folder where the data file is stored.\n\n# Use a relative path to import the data. \n\ndiabetes_df &lt;- read.csv(\"./FinalProjectRawData/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\ndiabetes_df &lt;- as_tibble(diabetes_df)\n\ndiabetes_df\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nWe will then check for missingness,\n\nlist(\"Number of missing values in data set\" = sum(is.na(diabetes_df)))\n\n$`Number of missing values in data set`\n[1] 0\n\n\nWe see that there are no missing values throughout the data set so we can proceed with converting the categorical variables to factors and provide meaningful names to variables if needed.\n\n# Convert variables to factors, replace with meaningful names\n\ndiabetes_df &lt;- diabetes_df |&gt;\n  dplyr::rename(\"HasDiabetes\" = Diabetes_binary, \n         \"HighCholesterol\" = HighChol, \n         \"CholesterolChecked\" = CholCheck, \n         \"ConsumesFruits\" = Fruits, \n         \"ConsumesVeggies\" = Veggies, \n         \"HeavyAlcoholUse\" = HvyAlcoholConsump,\n         \"HasHealthcare\" = AnyHealthcare,\n         \"ExpensiveTreatment\" = NoDocbcCost,\n         \"GeneralHealth\" = GenHlth,\n         \"BadMentalHealth\" = MentHlth,\n         \"BadPhysicalHealth\" = PhysHlth,\n         \"DifficultyWalking\" = DiffWalk) |&gt;\n  mutate(across(c(1:4, 6:15, 18:22), as.factor))\n\nstr(diabetes_df)\n\ntibble [253,680 × 22] (S3: tbl_df/tbl/data.frame)\n $ HasDiabetes         : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighCholesterol     : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholesterolChecked  : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 2 2 1 2 1 1 ...\n $ ConsumesFruits      : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 2 2 1 1 2 1 ...\n $ ConsumesVeggies     : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HeavyAlcoholUse     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HasHealthcare       : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ ExpensiveTreatment  : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GeneralHealth       : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 5 3 5 2 2 2 3 3 5 2 ...\n $ BadMentalHealth     : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ BadPhysicalHealth   : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DifficultyWalking   : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Factor w/ 13 levels \"1\",\"2\",\"3\",\"4\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Factor w/ 8 levels \"1\",\"2\",\"3\",\"4\",..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\nReview of our data frame structure shows that we have successfully renamed our variables and converted the categorical variables to factors. It is important to note that while BadMentalHealth and BadPhysicalHealth are categorical responses in the survey, the responses are discrete over a range of possible entries (1 to 30) makes them candidates for numeric analysis.\nNext we will generate a summary of the variables to provide insight on what we can expect to see during our EDA.\n\nprint(list(\"Summary of Diabetes Data Frame Variables\"=summary(diabetes_df)))\n\n$`Summary of Diabetes Data Frame Variables`\n HasDiabetes HighBP     HighCholesterol CholesterolChecked      BMI       \n 0:218334    0:144851   0:146089        0:  9470           Min.   :12.00  \n 1: 35346    1:108829   1:107591        1:244210           1st Qu.:24.00  \n                                                           Median :27.00  \n                                                           Mean   :28.38  \n                                                           3rd Qu.:31.00  \n                                                           Max.   :98.00  \n                                                                          \n Smoker     Stroke     HeartDiseaseorAttack PhysActivity ConsumesFruits\n 0:141257   0:243388   0:229787             0: 61760     0: 92782      \n 1:112423   1: 10292   1: 23893             1:191920     1:160898      \n                                                                       \n                                                                       \n                                                                       \n                                                                       \n                                                                       \n ConsumesVeggies HeavyAlcoholUse HasHealthcare ExpensiveTreatment GeneralHealth\n 0: 47839        0:239424        0: 12417      0:232326           1:45299      \n 1:205841        1: 14256        1:241263      1: 21354           2:89084      \n                                                                  3:75646      \n                                                                  4:31570      \n                                                                  5:12081      \n                                                                               \n                                                                               \n BadMentalHealth  BadPhysicalHealth DifficultyWalking Sex       \n Min.   : 0.000   Min.   : 0.000    0:211005          0:141974  \n 1st Qu.: 0.000   1st Qu.: 0.000    1: 42675          1:111706  \n Median : 0.000   Median : 0.000                                \n Mean   : 3.185   Mean   : 4.242                                \n 3rd Qu.: 2.000   3rd Qu.: 3.000                                \n Max.   :30.000   Max.   :30.000                                \n                                                                \n      Age        Education      Income     \n 9      :33244   1:   174   8      :90385  \n 10     :32194   2:  4043   7      :43219  \n 8      :30832   3:  9478   6      :36470  \n 7      :26314   4: 62750   5      :25883  \n 11     :23533   5: 69910   4      :20135  \n 6      :19819   6:107325   3      :15994  \n (Other):87744              (Other):21594  \n\n\nReeview of the data set summary shows that BMI appears to be centered with outliers that need investigated for removal. With median values of “0” over a range of 1-30, both BadMentalHealth and BadPhysicalHealth have a high tendency response of “0” in the survey. HighBP, HighCholesterol, Smoker, GeneralHealth, Sex and Age each appear to have response ratios near 1:1. The remaining binary variables of CholesterolChecked, Stroke, HeartDiseaseorAttack, PhysActivity, ConsumesFruits, ConsumesVeggies, HeavyAlcoholUse, HasHealthcare, ExpensiveTreatment, and DifficultyWalking each heavily favor one response over the other. The variables of Education and Income both show an increase in responses as the category range values increase.\n\n\n\nThe first variable we will investigate is BMI. Review of the variable summary information shows\n\nprint(list(\"Summary of BMI\"=summary(diabetes_df$BMI)))\n\n$`Summary of BMI`\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   24.00   27.00   28.38   31.00   98.00 \n\n\nthat the data has a Q1 value of 24, Q3 value of 31, Median of 27, and Mean of 28.38. This tells us the data is tightly grouped together with BMI values of less than 13.5 and greater than 41.5 as outliers. These outliers include our Min of 12 and Max of 98. A histogram of BMI shows\n\nBMI_histogram &lt;- ggplot(diabetes_df, aes(BMI)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  labs(title=\"Histogram of BMI\", y=\"Density (# of PTs)\") +\n  scale_x_continuous(breaks = seq(0, 100, by = 10)) +\n  stat_function(fun=dnorm, args=list(mean=mean(diabetes_df$BMI), sd=sd(diabetes_df$BMI))) \n\nBMI_histogram\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nthat the data follows a generally normal distribution with some skewness to the right. The histogram also visually confirms there are outleirs in the BMI ranges of 12-13 and 50-98.\n\nlowrange &lt;- sum(diabetes_df$BMI &lt; 13.5)\n\nhighrange &lt;- sum(diabetes_df$BMI &gt; 41.5)\n\nBMIoutliers &lt;-sum(lowrange, highrange)\n\nprint(list(\"Number of BMI Oultiers\"=BMIoutliers))\n\n$`Number of BMI Oultiers`\n[1] 9847\n\nprint(list(\"BMI Outlier portion of data set\"=BMIoutliers/nrow(diabetes_df)))\n\n$`BMI Outlier portion of data set`\n[1] 0.03881662\n\n\nWe identified there are total of 9847 observation that are outliers when using 1.5 the IQR as our defining statistic. This accounts for over 3.88% of the data set, which is a higher than we’re willing to remove. Because the histogram shows there is close to a 0% probability of entries starting near a BMI of 50,\n\nlowrange &lt;- sum(diabetes_df$BMI &lt; 13.5)\n\nhighrange &lt;- sum(diabetes_df$BMI &gt; 50)\n\nBMIoutliers &lt;-sum(lowrange, highrange)\n\nprint(list(\"Number of BMI Oultiers\"=BMIoutliers))\n\n$`Number of BMI Oultiers`\n[1] 2202\n\nprint(list(\"BMI Outlier portion of data set\"=BMIoutliers/nrow(diabetes_df)))\n\n$`BMI Outlier portion of data set`\n[1] 0.008680227\n\n\nwe udpated the parametrs and found there to be 2202 outlier observations which account for less than 1% of the data set. It would be reasonable to remove these observations from the data set before prediction model training.\nRemoving the outliers then splitting the BMI histogram with respect to HasDiabetes shows\n\ndiabetes_df &lt;- diabetes_df |&gt;\n  filter(BMI &gt;= 13.5) |&gt;\n  filter(BMI &lt;= 50 )\n\nBMI_by_histplot &lt;- ggplot(diabetes_df, aes(BMI)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  labs(title=\"Histogram of BMI by HasDiabetes\", y=\"Density (# of PTs)\") +\n  scale_x_continuous(breaks = seq(0, 100, by = 10)) +\n  stat_function(fun=dnorm, args=list(mean=mean(diabetes_df$BMI), sd=sd(diabetes_df$BMI))) +\n  facet_grid(~HasDiabetes)\n\nBMI_by_histplot\n\n\n\n\n\n\n\n\nthat the distribution of observations in the “No” HasDiabetes group has a center near a BMI of 25 while the “Yes” HasDiabetes group has a center near a BMI of 30. This change of BMI density distributions between the HasDiabetes groups indicates that BMI appears to be a suitable variable for prediction modeling.\nThe next variable we will investigate is HighPB. Review of the variable summary information shows\n\nprint(list(\"Summary of HighBP\"=summary(diabetes_df$HighBP)))\n\n$`Summary of HighBP`\n     0      1 \n143994 107484 \n\n\nthere are greater number of observations of PTs not having high blood pressure to those that do in a ratio of approximately 7:5. This is fairly close to being 1:1, so any differences between the HasDiabetes categories would be due to interactions between the variables.\nUsing a bar plot to observe HighBP with respect to HasDiabetes shows\n\nHighBP_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, HighBP) |&gt;\n  summarize(count = n())\n\nHighPB_plot &lt;- ggplot(HighBP_by, aes(HasDiabetes, count, fill=HighBP)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title = \"Bar Plot of HighBP by HasDiabetes\", x=\"Has Diabetes\", y=\"# of PTs\")\n\nHighPB_plot\n\n\n\n\n\n\n\n\nthat the majority of PTs with diabetes have high blood pressure while the majority of PTs without diabetes do not have high blood pressure. The observations in the “No” HasDiabetes category have a “No” to “Yes” HighBP ratio of approximately 7:4 (near 2:1). The observations in the “Yes” HasDiabetes category have a “No” to “Yes” HighBP ratio of approximately 4:13 (near 1:3). This flip in ratios between the HasDiabetes groups suggests that there is a relationship between HasDiabetes and HighBP where a PT with high blood pressure will also have diabetes.\nThe next variable that we will investigate is HighCholesterol. Review of the variable summary information shows\n\nprint(list(\"Summary of HighCholesterol\"=summary(diabetes_df$HighCholesterol)))\n\n$`Summary of HighCholesterol`\n     0      1 \n144867 106611 \n\n\nthere is approximately a 14:11 “No” to “Yes” ratio of HighCholesterol observations. This distribution is fairly close to being 1:1, so any differences between the HasDiabetes categories would be due to interactions between the variables.\nUsing a bar plot to observe HighCholesterol with respect to HasDiabetes we see\n\nHighCholesterol_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, HighCholesterol) |&gt;\n  summarize(count = n())\n\nHighCholesterol_plot &lt;- ggplot(HighCholesterol_by, aes(HasDiabetes, count, fill=HighCholesterol)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of HighCholesterol by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nHighCholesterol_plot\n\n\n\n\n\n\n\n\nthat the ratios change between the HasDiabetes groups. While the ratios favor low cholesterol in the group without diabetes, they favor high cholesterol levels in the group with diabetes. There is approximately a 13:8 (nears 2:1) ratio of “No” to “Yes” HighCholesterol responses in the “No” HasDiabetes group. There is approximately a 11:23 (nears 1:2) ratio of “No” to “Yes” HighCholesterol responses in the “Yes” HasDiabetes group. The shift in ratios between the groups trending towards PTs with diabetes having high cholesterol suggests that there is a relationship between HasDiabetes and HighCholesterol where someone with high cholesterol will also have diabetes.\nThe next variable we will investigate is CholesterolChecked. Review of the variable summary information shows\n\nprint(list(\"Summary of CholesterolChecked\"=summary(diabetes_df$CholesterolChecked)))\n\n$`Summary of CholesterolChecked`\n     0      1 \n  9404 242074 \n\n\nthat the overwhelming majority of PTs had their cholesterol checked at a ratio of 9:242 (nears 1:27) when compared to those who did not. Any differences between the HasDiabetes groups may be heavily influenced by the number of observations in each group more than the interaction between the variables. Review of a bar plot of CholesterolChecked with respect to HasDiabetes shows\n\nCholesterolChecked_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, CholesterolChecked) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nCholesterolChecked_plot &lt;- ggplot(CholesterolChecked_by, aes(HasDiabetes, count, fill=CholesterolChecked)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of CholesterolChecked by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\")\n\nCholesterolChecked_plot\n\n\n\n\n\n\n\n\nthat the group of “Yes” CholesterolChecked responses dominates both categories of HasDiabetes. The “No” HasDiabetes group has a “No” to “Yes” CholesterolChecked ratio of approximately 9:208 (nears 1:23). The “Yes” HasDiabetes group has a “No” to “Yes” CholesterolChecked ratio of approximately 1:172. This information suggests that there is a relationship between the variables where someone with diabetes has a high probability of having their cholesteroal checked every five years, which is not what we’re trying to predict. CholesterolChecked does not appear to be a good variable for use in prediction modeling.\nThe next variable we’re going to analyze is Smoker. Review of the variable summary shows that\n\nprint(list(\"Summary of Smoker\"=summary(diabetes_df$Smoker)))\n\n$`Summary of Smoker`\n     0      1 \n139967 111511 \n\n\nthe ratio of smokers to non-smokers in the study is approximately 14:11. This is distribution between the groups is fairly close to a 1:1 distribution. Any differences between the HasDiabetes groups will be due to interactions between the two variables. Reviewing the bar plot of Smoker with respect to HasDiabetes shows\n\nSmoker_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Smoker) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nSmoker_plot &lt;- ggplot(Smoker_by, aes(HasDiabetes, count, fill=Smoker)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Smoker by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nSmoker_plot\n\n\n\n\n\n\n\n\nshows that the “No” HasDiabetes group has a “No” to “Yes” Smoker distribution of approximately 62:47 which is closer to a 1:1 distribution than a 2:1 distribution. The “Yes” HasDiabetes group has a “No” to “Yes” Smoker distribution of approximately 17:18 which also nears a 1:1 distribution. While the ratios shifts indicate that smokers have a higher likelihood of having diabetes than non smokers, neither Smoker group separates itself from the variable in both HasDiabetes categories. Smoker is not a suitable variable for inclusion in our prediction model.\nThe next variable we are going to analyze is Stroke. Review of the variable summary shows\n\nprint(list(\"Summary of Stroke\"=summary(diabetes_df$Stroke)))\n\n$`Summary of Stroke`\n     0      1 \n241295  10183 \n\n\nthat Stroke has a 24:1 “No” to “Yes” ratio. This suggests that any differences between the HasDiabetes groups may be heavily influenced by the number of observations in each group more than the interaction between the variables. Review of a bar plot for Stroke with respect to HasDiabetes shows\n\nStroke_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Stroke) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nStroke_plot &lt;- ggplot(Stroke_by, aes(HasDiabetes, count, fill=Stroke)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Stroke by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nStroke_plot\n\n\n\n\n\n\n\n\nthat the Stroke “No” group is prominent in both groups of HasDiabetes. The “No” HasDiabetes group has a “No to”Yes” Stroke ratio of approximately 211:7 which is close to 30:1. The “Yes” HasDiabetes group has a “No to”Yes” Stroke ratio of approximately 32:3 which is close to 11:1. While still favoring the “No” Stroke group, the reduction in ratios indicates that there is a relationship between Stroke and HasDiabetes. However, the relationship appears to be where someone who had a stroke is likely to also have diabetes, but not someone who has diabetes is likely to have had a stroke. Stroke does not appear to be suitable for inclusion in the prediction model.\nThe next variable we are going to analyze is HeartDisesaseorAttack. Summary information shows\n\nprint(list(\"Summary of HeartDiseaseorAttack\"=summary(diabetes_df$HeartDiseaseorAttack)))\n\n$`Summary of HeartDiseaseorAttack`\n     0      1 \n227838  23640 \n\n\nthat the ratio between the “No” and “Yes” HeartDiseaseorAttack groups is approximately 23:2 which is slightly larger than 11:1. Review of a bar plot for HeartDiseaseorAttack with respect to HasDiabetes shows\n\nHeartDiseaseorAttack_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, HeartDiseaseorAttack) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nHeartDiseaseorAttack_plot &lt;- ggplot(HeartDiseaseorAttack_by, aes(HasDiabetes, count, fill=HeartDiseaseorAttack)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of HeartDiseaseorAttack by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nHeartDiseaseorAttack_plot\n\n\n\n\n\n\n\n\nshows that the “No” HeartDiseaseorAttack group is noticably larger than the “Yes” group for each category of HasDiabetes. The “No” HeartDisease group has a “No” to “Yes” HeartDiseaseorAttack ratio of approximately 10:1. The “Yes” HeartDisease group has a “No” to “Yes” HeartDiseaseorAttack ratio of 27:7 which nears 4:1. The ratio change between the HasDiabetes groups suggests that there is a relationship between the variables. However, this relationship appears to be one where someone having diabetes can be used to predict if they have heart disease or have experienced a heart attack, which is not what we’re trying to predict. HeartDiseaseorAttack does not appear to be suitable for inclusion in the prediction model.\nThe next variable we will summarize is PhysActivity. Review of the variable summary information shows\n\nprint(list(\"Summary of PhysicalActivity\"= summary(diabetes_df$PhysActivity)))\n\n$`Summary of PhysicalActivity`\n     0      1 \n 60722 190756 \n\n\nthe ratio of “No” to “Yes” responses is approximately 6:19 which is slightly less than 1:3. This suggests that differences between the HasDiabetes groups may be influenced by the number of “Yes” PhysActivity observations. Review of a bar plot of PhysActivity with respect to HasDiabetes shows\n\nPhysActivity_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, PhysActivity) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nPhysActivity_plot &lt;- ggplot(PhysActivity_by, aes(HasDiabetes, count, fill=PhysActivity)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of PhysActivity by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nPhysActivity_plot\n\n\n\n\n\n\n\n\nPhysActivity is greater in both HasDiabetes groups. The “No” HeartDisease group has a “No” to “Yes” PhysActivity ratio of approximately 5:17 which is slightly less than 1:3. The “Yes” HeartDisease group has a “No” to “Yes” PhysActivity ratio of approximately 13:22 which is slightly greater than 1:2. This change in ratios shows that there is a relationship between the variables. The relationship appears to be one where someone having diabetes can be used to predict if they participate in physical activity or not, which is not what we’re trying to predict. PhysActivity does not appear to be suitable for inclusion in the prediction model.\nThe next variable we’re going to summarize is ConsumesFruits. Review of the variable summary information shows\n\nprint(list(\"Summary of ConsumesFruits\"=summary(diabetes_df$ConsumesFruits)))\n\n$`Summary of ConsumesFruits`\n     0      1 \n 91734 159744 \n\n\nthat ConsumesFruits has a “No” to “Yes” ratio of approximately 9:16, which is slightly greater than a 1:2 distribution. Review of a bar graph of ConsumesFruits with respect to HasDiabetes shows\n\nConsumesFruits_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, ConsumesFruits) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nConsumesFruits_plot &lt;- ggplot(ConsumesFruits_by, aes(HasDiabetes, count, fill=ConsumesFruits)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of ConsumesFruits by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nConsumesFruits_plot\n\n\n\n\n\n\n\n\nthat both groups of HasDiabetes have a greater number of “Yes” ConsumesFruits observations. The “No” HasDiabetes group has a “No” to “Yes” ConsumesFruits ratio of approximately 8:14 which is slightly greater than 1:2. The “Yes” HasDiabetes group has a “No” to “Yes” ConsumesFruits ratio of approximately 15:21 which is slightly greater than 2:3. This change in ratios shows that there is a relationship between the variables. However, the relationship appears to be one where someone having diabetes can be used to predict if they eat fruits or not, which is not what we’re tyring to predict. ConsumesFruits does not appear to be suitable for inclusion in the prediction model.\nThe next variabel we’re going to summarize is **ConsumesVeggies*. Review of the variable summary information shows\n\nprint(list(\"Summary of ConsumesVeggies\"=summary(diabetes_df$ConsumesVeggies)))\n\n$`Summary of ConsumesVeggies`\n     0      1 \n 47284 204194 \n\n\nthat the proporiton of “No” to “Yes” responses is approximately 5:21 which is slightly less than 1:4. Review of a bar plot of ConsumesVeggies with respect to HasDiabetes shows\n\nConsumesVeggies_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, ConsumesVeggies) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nConsumesVeggies_plot &lt;- ggplot(ConsumesVeggies_by, aes(HasDiabetes, count, fill=ConsumesVeggies)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of ConsumesVeggies by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nConsumesVeggies_plot\n\n\n\n\n\n\n\n\nthat the “Yes” ConsumesVeggies observations are substantially greater in both categories of HasDiabetes. The “No” HasDiabetes group has a “No” to “Yes” ConsumesVeggies ratio of approximately 2:9 which is slightly less than 1:4. The “Yes” HasDiabetes group has a “No” to “Yes” ConsumesVeggies ratio of approximately 1:3. The change in ratios between the HasDiabetes groups shows that there is a relationship between the variables. However, the relationship appears to be one where someone having diabetes can be used to predict if they eat vegetables or not, which is not what we’re trying to predict. ConsumesVeggies does not appear to be suitable for inclusion in the prediction model.\nThe next variable we’re going to summarize is HeavyAlcoholUse. Review of the variable summary information shows\n\nprint(list(\"Summary of HeavyAlcoholUse\"=summary(diabetes_df$HeavyAlcoholUse)))\n\n$`Summary of HeavyAlcoholUse`\n     0      1 \n237283  14195 \n\n\nshows that the ratio of “No” to “Yes” responses is approximately 24:1. This suggests that any differences between the HasDiabetes groups may be influenced by the number of observations. Reviewing the bar plot of HeavyAlcoholUse with respect to **HeartDisease* shows\n\nHeavyAlcoholUse_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, HeavyAlcoholUse) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nHeavyAlcoholUse_plot &lt;- ggplot(HeavyAlcoholUse_by, aes(HasDiabetes, count, fill=HeavyAlcoholUse)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of HeavyAlcoholUse by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nHeavyAlcoholUse_plot\n\n\n\n\n\n\n\n\nthat the “No” HeavyAlcoholUse responses constitute the majority of observations in both HasDiabetes groups. The “No” HasDiabetes group has a “No” to “Yes” HeavyAlcoholUse ratio of approximately 205:13 which reduces close to 16:1. The “Yes” HasDiabetes group has a “No” to “Yes” HeavyAlcoholUse ratio of approximately 345:8 which reduces close to 43:1. The change in ratios shows that there is a relationship between the variables. However, the relationship appears to be one where someone having diabetes can be used to predict if they consume alcohol heavily, which is not what we’re trying to predict. HeavyAlcoholUse does not appear to be suitable for inclusion in the prediction model.\nThe next variable we’re going to summarize is HasHealthcare. Review of the variable summary information\n\nprint(list(\"Summary of HasHealthcare\"=summary(diabetes_df$HasHealthcare)))\n\n$`Summary of HasHealthcare`\n     0      1 \n 12246 239232 \n\n\nshows that the ratio of “No” to “Yes” observations is approximately 12:241 which is close to a 1:20 ratio. This is suggestive that any differences between the HasDiabetes categories may be heavily influenced by the number of responses in each. Reviewing the bar chart of HasHealthcare with respect to HasDiabetes shows\n\nHasHealthcare_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, HasHealthcare) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nHasHealthcare_plot &lt;- ggplot(HasHealthcare_by, aes(HasDiabetes, count, fill=HasHealthcare)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of HasHealthcare by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nHasHealthcare_plot\n\n\n\n\n\n\n\n\nthe “Yes” HasHealthcare observations are substantially higher than the “No” responses for both categories of **HasDiabetes*. The “No” HasDiabetes group has a “No” to “Yes” HasHealthcare ratio of approximately 11:207 which reduces close to 1:19. The “Yes” HasDiabetes group has a “No” to “Yes” HasHealthcare ratio of approximately 14:339 which reduces close to 1:24. The ratios in both groups of HasDiabetes do not differ much from the overall HasHealthcare variable ratio of 1:20. HasHealthcare does not appear to be suitable for inclusion in the prediction model.\nThe next variable we’re going to summarize is ExpensiveTreatment. Review of the variable summary information\n\nprint(list(\"Summary of ExpensiveTreatment\"=summary(diabetes_df$ExpensiveTreatment)))\n\n$`Summary of ExpensiveTreatment`\n     0      1 \n230457  21021 \n\n\nthat the ration of “No” to “Yes” observations is approximately 232:21 which reduces near 11:1. This is suggestive that any differences between the HasDiabetes categories may be heavily influenced by the number of repsonses in each. Reviwing the bar chart of ExpensiveTreatment with respect to HasDiabetes shows\n\nExpensiveTreatment_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, ExpensiveTreatment) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nExpensiveTreatment_plot &lt;- ggplot(ExpensiveTreatment_by, aes(HasDiabetes, count, fill=ExpensiveTreatment)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of ExpensiveTreatment by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nExpensiveTreatment_plot\n\n\n\n\n\n\n\n\nthe “No” observations are substantially greater than the “Yes” responses for both categories of HasDiabetes. The “No” HasDiabetes group has a “No” to “Yes” ExpensiveTreatment ratio of approximately 201:17 which reduces close to 12:1. The “Yes” HasDiabetes group has a “No” to “Yes” ExpensiveTreatment ratio of approximately 316:37 which reduces close to 9:1. There appears to be a relationship between the variables where someone with diabetes has a greater likelihood of not seeking treatment due to the cost, but that is not what we’re trying to predict. ExpensiveTreatment does not appear to be suitbale for inclusion in the prediciton model.\nThe next variable we’re going to summarize is GeneralHealth. Review of the variable summary information\n\nprint(list(\"Summary of GeneralHealth\"=summary(diabetes_df$GeneralHealth)))\n\n$`Summary of GeneralHealth`\n    1     2     3     4     5 \n45159 88704 74967 30900 11748 \n\n\nshows that the majority of patients reported being in “Excellent” to “Good” health. Reviewing the bar chart of GeneralHealth with respect to HasDiabetes shows\n\nGeneralHealth_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, GeneralHealth) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nGeneralHealth_plot &lt;- ggplot(GeneralHealth_by, aes(HasDiabetes, count, fill=GeneralHealth)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of GeneralHealth by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nGeneralHealth_plot\n\n\n\n\n\n\n\n\nthat there is a shift in the reporting tendencies between the HasDiabetes categories. The “No” HasDiabetes group has a central tendency of “Very Good” followed by “Good” GeneralHealth responses while the “Yes” HasDiabetes group has a central tendency of “Good” followed by “Fair” GeneralHealth responses\n\nNoDiabetes_high_three &lt;- sum(GeneralHealth_by$count[1:3])/sum(GeneralHealth_by$count[1:5])\nNoDiabetes_fourth &lt;- GeneralHealth_by$count[4]/sum(GeneralHealth_by$count[1:5])\n\nYesDiabetes_high_three &lt;- sum(GeneralHealth_by$count[7:9])/sum(GeneralHealth_by$count[6:10]) \nYesDiabetes_fourth &lt;- GeneralHealth_by$count[10]/sum(GeneralHealth_by$count[6:10])\n\nprint(list(\"GeneralHealth 1 through 3, HasDiabetes in 'No'\"=NoDiabetes_high_three, \"GeneralHealth 4, HasDiabetes in 'No'\"=NoDiabetes_fourth, \"GeneralHealth 2 through 4, HasDiabetes in 'Yes'\"=YesDiabetes_high_three, \"GeneralHealth 5, HasDiabetes in 'Yes'\"=NoDiabetes_fourth))\n\n$`GeneralHealth 1 through 3, HasDiabetes in 'No'`\n[1] 0.8674027\n\n$`GeneralHealth 4, HasDiabetes in 'No'`\n[1] 0.09865936\n\n$`GeneralHealth 2 through 4, HasDiabetes in 'Yes'`\n[1] 0.8406513\n\n$`GeneralHealth 5, HasDiabetes in 'Yes'`\n[1] 0.09865936\n\n\nReview of the GeneralHealth response proportions in the “No” HasDiabetes group shows that over 86.59% of the observations reported having a GeneralHealth status of “Good” or better for the three highest responses with the 4th highest response of “Fair” accounting for another 9.98% of group responses. Review of the GeneralHealth response proportions in the “Yes” HasDiabetes shows that over 83.82% of the observations reported having a GeneralHealth status of “Very Good” to “Fair” for the three highest responses with the fourth highest response of “Poor” accounting for another 9.98% of group responses. The shift in GeneralHealth tendency between the HasDiabetes groups suggests that there is a relationship between the variables. GeneralHealth appears to be a suitable variable for prediciton modeling.\nThe next variable we’re going to summarize is BadMentalHealth. Review of the variable summary information\n\nprint(list(\"Summary of BadMentalHealth\"=summary(diabetes_df$BadMentalHealth)))\n\n$`Summary of BadMentalHealth`\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   3.155   2.000  30.000 \n\n\nshows that there is a tendency for the PTs to report that their mental health was not good for zero days.\nReview of a histogram of the variable along with histograms of MentalHealth with respect to HasDiabetes shows\n\nBadMentalHealth_histogram &lt;- ggplot(diabetes_df, aes(BadMentalHealth)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  scale_x_continuous(breaks = seq(0, 30, by = 5)) +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.05)) +\n  labs(title=\"Histogram of BadMentalHealth\", x=\"# of Days Mental Health Not Good\", y=\"Density (# of PTs)\")\n\nBadMentalHealth_histogram\n\n\n\n\n\n\n\nBadMentalHealth_by_histplot &lt;- ggplot(diabetes_df, aes(BadMentalHealth)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  labs(title=\"Histogram of BadMentalHealth by HasDiabetes\", y=\"Density (# of PTs)\") +\n  scale_x_continuous(breaks = seq(0, 30, by = 5)) +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.05)) +\n  facet_grid(~HasDiabetes)\n\nBadMentalHealth_by_histplot\n\n\n\n\n\n\n\n\nthat the majority of the observations in both categories of HasDiabetes follow the similar trend where the majority of non-zero values in BadMentalHealth are in the range of 1-10 days. After remvoing the zero values for analysis, we compared the proportions of 10-day BadMentalHealth ranges between the HasDiabetes groups.\n\nBadMentalHealth_by_nozero &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, BadMentalHealth) |&gt;\n  filter(BadMentalHealth &gt;= 1) |&gt;\n  summarize(count=n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nprint(BadMentalHealth_by_nozero, n=60)\n\n# A tibble: 60 × 3\n# Groups:   HasDiabetes [2]\n   HasDiabetes BadMentalHealth count\n   &lt;fct&gt;                 &lt;dbl&gt; &lt;int&gt;\n 1 0                         1  7679\n 2 0                         2 11465\n 3 0                         3  6403\n 4 0                         4  3259\n 5 0                         5  7729\n 6 0                         6   819\n 7 0                         7  2672\n 8 0                         8   524\n 9 0                         9    78\n10 0                        10  5256\n11 0                        11    38\n12 0                        12   330\n13 0                        13    32\n14 0                        14   957\n15 0                        15  4422\n16 0                        16    73\n17 0                        17    42\n18 0                        18    76\n19 0                        19    12\n20 0                        20  2654\n21 0                        21   178\n22 0                        22    50\n23 0                        23    30\n24 0                        24    27\n25 0                        25   896\n26 0                        26    37\n27 0                        27    66\n28 0                        28   268\n29 0                        29   128\n30 0                        30  9183\n31 1                         1   788\n32 1                         2  1481\n33 1                         3   900\n34 1                         4   476\n35 1                         5  1186\n36 1                         6   158\n37 1                         7   391\n38 1                         8   106\n39 1                         9    12\n40 1                        10  1033\n41 1                        11     3\n42 1                        12    62\n43 1                        13     8\n44 1                        14   190\n45 1                        15   989\n46 1                        16    12\n47 1                        17    11\n48 1                        18    19\n49 1                        19     4\n50 1                        20   639\n51 1                        21    46\n52 1                        22    10\n53 1                        23     8\n54 1                        24     6\n55 1                        25   258\n56 1                        26     6\n57 1                        27    11\n58 1                        28    54\n59 1                        29    27\n60 1                        30  2664\n\nHasDiabetesNo_first_ten &lt;- list(\"HasDiabetesNo, BadMentalHealth, 1-10 days proprotion of data\" =sum(BadMentalHealth_by_nozero$count[1:10])/sum(BadMentalHealth_by_nozero$count[1:30]))\n\nHasDiabetesNo_middle_ten &lt;- list(\"HasDiabetesNo, BadMentalHealth, 11-20 days proprotion of data\" =sum(BadMentalHealth_by_nozero$count[11:20])/sum(BadMentalHealth_by_nozero$count[1:30]))\n\nHasDiabetesNo_last_ten &lt;- list(\"HasDiabetesNo, BadMentalHealth, 21-30 days proprotion of data\" =sum(BadMentalHealth_by_nozero$count[21:30])/sum(BadMentalHealth_by_nozero$count[1:30]))\n\nHasDiabetesYes_first_ten &lt;- list(\"HasDiabetesYes, BadMentalHealth, 1-10 days proprotion of data\"=sum(BadMentalHealth_by_nozero$count[31:40])/sum(BadMentalHealth_by_nozero$count[31:60]))\n\nHasDiabetesYes_middle_ten &lt;- list(\"HasDiabetesYes, BadMentalHealth, 11-20 days proprotion of data\"=sum(BadMentalHealth_by_nozero$count[41:50])/sum(BadMentalHealth_by_nozero$count[31:60]))\n\nHasDiabetesYes_last_ten &lt;- list(\"HasDiabetesYes, BadMentalHealth, 20-30 days proprotion of data\"=sum(BadMentalHealth_by_nozero$count[51:60])/sum(BadMentalHealth_by_nozero$count[31:60]))\n\nprint(c(HasDiabetesNo_first_ten, HasDiabetesYes_first_ten, HasDiabetesNo_middle_ten, HasDiabetesYes_middle_ten, HasDiabetesNo_last_ten, HasDiabetesYes_last_ten))\n\n$`HasDiabetesNo, BadMentalHealth, 1-10 days proprotion of data`\n[1] 0.7017726\n\n$`HasDiabetesYes, BadMentalHealth, 1-10 days proprotion of data`\n[1] 0.5650632\n\n$`HasDiabetesNo, BadMentalHealth, 11-20 days proprotion of data`\n[1] 0.1320833\n\n$`HasDiabetesYes, BadMentalHealth, 11-20 days proprotion of data`\n[1] 0.1675895\n\n$`HasDiabetesNo, BadMentalHealth, 21-30 days proprotion of data`\n[1] 0.1661441\n\n$`HasDiabetesYes, BadMentalHealth, 20-30 days proprotion of data`\n[1] 0.2673473\n\n\nThe comparisons showed that the “No” HasDiabetes group had a 70% of their BadMentalHealth responses in the 1-10 day range while the “Yes” HasDiabetes group only had a 56.2% of their BadMentalHealth responses in the same range. Likewise, the “Yes” HasDiabetes group had higher percentages of responses in both the 11-20 day and 21-30 day BadMentalHealth ranges than the “No” HasDiabetes group. The change in “0” response density between the HasDiabetes groups appears to have shifted to longer periods of PTs with diabetes experiencing mental health that is not good. This shows that there is a relationship between the variables. BadMentalHealth appears to be suitable for prediction modeling.\nThe next variable we’re going to summarize is BadPhysicalHealth. Review of the variable summary information\n\nprint(list(\"Summary of PhysicalHealth\"=summary(diabetes_df$BadPhysicalHealth)))\n\n$`Summary of PhysicalHealth`\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   4.194   3.000  30.000 \n\n\nshows that like BadMentalHealth, the vast majority of BadPhysicalHealth responses provided were “0”. Reviewing a density plot histogram of BadPhysicalHealth along with histogram plots of BadPhysicalHealth with respect to HasDiabetes show\n\nBadPhysicalHealth_histogram &lt;- ggplot(diabetes_df, aes(BadPhysicalHealth)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  scale_x_continuous(breaks = seq(0, 30, by = 5)) +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.05)) +\n  labs(title=\"Histogram of BadPhysicalHealth\", x=\"# of Days Mental Health Not Good\", y=\"Density (# of PTs)\")\n\nBadPhysicalHealth_histogram\n\n\n\n\n\n\n\nBadPhysicalHealth_by_histplot &lt;- ggplot(diabetes_df, aes(BadPhysicalHealth)) +\n  geom_histogram(aes(y=..density..), binwidth=1, fill=\"#CC0000\", color=\"lightgray\") +\n  labs(title=\"Histogram of BadPhysicalHealth by HasDiabetes\", y=\"Density (# of PTs)\") +\n  scale_x_continuous(breaks = seq(0, 30, by = 5)) +\n  scale_y_continuous(breaks = seq(0, 30, by = 0.05)) +\n  facet_grid(~HasDiabetes)\n\nBadPhysicalHealth_by_histplot\n\n\n\n\n\n\n\n\nthat like with BadMentalHealth, the density of the BadPhysicalHealth “0” response changes between the HasDiabetes groups. The general trend is that when compared to the “No” HasDiabetes group the “Yes” HasDiabetes densities in the 0-7 day range are lower and then overall higher among the remaining groups. Comparing the proportions of the 0-7day ranges to the 8-30 day ranges\n\nBadPhysicalHealth_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, BadMentalHealth) |&gt;\n  summarize(count=n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nBadPhysHlth_DiabetesNo_firstSeven &lt;- list(\"BadPhysHealth 1-7 Days, Diabetes in 'No'\" = sum(BadPhysicalHealth_by$count[1:8])/sum(BadPhysicalHealth_by$count[1:31]))\n\nBadPhysHlth_DiabetesNo_lastTwentyThree &lt;- list(\"BadPhysHealth 8-30 Days, Diabetes in 'No'\" = sum(BadPhysicalHealth_by$count[9:31])/sum(BadPhysicalHealth_by$count[1:31]))\n\nBadPhysHlth_DiabetesYes_firstSeven &lt;- list(\"BadPhysHealth 1-7 Days, Diabetes in 'Yes'\" = sum(BadPhysicalHealth_by$count[32:39])/sum(BadPhysicalHealth_by$count[32:62]))\n\nBadPhysHlth_DiabetesYes_lastTwentyThree &lt;- list(\"BadPhysHealth 8-30 Days, Diabetes in 'Yes'\" = sum(BadPhysicalHealth_by$count[40:62])/sum(BadPhysicalHealth_by$count[32:62]))\n\nprint(c(BadPhysHlth_DiabetesNo_firstSeven, BadPhysHlth_DiabetesYes_firstSeven, BadPhysHlth_DiabetesNo_lastTwentyThree, BadPhysHlth_DiabetesYes_lastTwentyThree))\n\n$`BadPhysHealth 1-7 Days, Diabetes in 'No'`\n[1] 0.8830596\n\n$`BadPhysHealth 1-7 Days, Diabetes in 'Yes'`\n[1] 0.8216564\n\n$`BadPhysHealth 8-30 Days, Diabetes in 'No'`\n[1] 0.1169404\n\n$`BadPhysHealth 8-30 Days, Diabetes in 'Yes'`\n[1] 0.1783436\n\n\nconfirms that the visual assessments are correct. The “No” HasDiabetes group is 6.4 percentage points higher in the BadPhysicalHealth 0-7 day range compared to the “Yes” HasDiabetes group. This change in HasDiabetes suggests that there is a relationship between the variables. **BadPhysicalHealth* appears to be suitable for prediction modeling.\nThe next variable we’re going to summarize is DifficultyWalking. Reviewing the variable summary\n\nprint(list(\"Summary of DifficultyWalking\"=summary(diabetes_df$DifficultyWalking)))\n\n$`Summary of DifficultyWalking`\n     0      1 \n209904  41574 \n\n\nshows that the majority of PTs reported “No” at a ratio of approximately 211:43, which reduces to slightly less than 5:1. This suggests that any differences in the HasDiabetes groups will be highly influenced by the number of PTs in each. Reviewing the bar plot of DifficultyWalking with respect to HasDiabetes shows\n\nDifficultyWalking_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, DifficultyWalking) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nDifficultyWalking_plot &lt;- ggplot(DifficultyWalking_by, aes(HasDiabetes, count, fill=DifficultyWalking)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of DifficultyWalking by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nDifficultyWalking_plot\n\n\n\n\n\n\n\n\nthat the majority of responses in both HasDiabetes groups fall under the “No” DifficultyWalking group. The observations in the “No” HasDiabetes category have a “No” to “Yes” DifficultyWalking ratio of approximately 188:30 which simplifies down to a ratio slightly greater than 6:1. The observations in the “Yes” HasDiabetes category have a “No” to “Yes” DifficultyWalking ratio of approximately 222:131 which simplifies down to closer to 2:1 than 1:1. The change in ratios suggests that there is a relationship between the variabels. However, the relationship appears to be one where someone having diabetes or not can help predict if they’ll have difficulty walking, but that is not what we’re trying to predict. DifficultyWalking does not appear to be a suitable variable for our prediction model.\nThe next variable we’re going to summarize is Sex. Reviewing the variable summary\n\nprint(list(\"Summary of Sex\"=summary(diabetes_df$Sex)))\n\n$`Summary of Sex`\n     0      1 \n140542 110936 \n\n\nshows that the distribution between the categories is almost uniform at a female to male ratio of approximately 142:111 which simplifies to a ratio slightly greater than 1:1. Reviewing the bar plot of Sex with respect to HasDiabetes shows\n\nSex_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Sex) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nSex_plot &lt;- ggplot(Sex_by, aes(HasDiabetes, count, fill=Sex)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Sex by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") \n\nSex_plot\n\n\n\n\n\n\n\n\nthat the number of females is greater in both HasDiabetes groups. The observations in the “No” HasDiabetes category have a “No” to “Yes” Sex ratio of approximately 124:95 which simplifies down to a ratio of slightly greater than 1:1. The observations in the “Yes” HasDiabetes category have a “No” to “Yes” Sex ratio of approximately 184:169 which also simplifies down to a ratio slightly greater than 1:1. The difference between the groups is minimal which suggests there is not a relationship between the variables. Sex does not appear suitable for our prediciton model.\nThe next variable we’re going to summarize is Age. Review of the variable summary\n\nprint(list(\"Summary of Age\"=summary(diabetes_df$Age)))\n\n$`Summary of Age`\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n 5659  7506 10947 13680 15963 19569 26002 30503 32959 31963 23451 15942 17334 \n\n\nshows that there is a trend where the number of responses increase in each category until group 9 (60-64), where they then decrease as from category 10 to 12 with group 13 being slightly higher than group 12. Reviewing histograms of Age with respect to HasDiabetes shows\n\nAge_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Age) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nAge_plot &lt;- ggplot(Age_by, aes(HasDiabetes, count, fill=Age)) +\n  geom_bar(stat = \"identity\", position = position_dodge2()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25, size=3) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Age by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\")\n\nAge_plot\n\n\n\n\n\n\n\n\nthat there a shift in distribution between the HasDiabetes groups. The “No” HasDiabetes group has central tendency around resonse “9” while the “Yes” HasDiabetes group has a central tendency around response “10”. Review of a summary table of the proportions between the HasDiabetes categories by groups shows\n\nAgeGrp &lt;- Age_by$Age[1:13]\n\nproportionNo &lt;- proportions(Age_by$count[1:13])\n\nproporitonYes &lt;-proportions(Age_by$count[14:26])\n\nAge_Diabetes &lt;- data.frame(AgeGrp, proportionNo, proporitonYes)\n\nAge_Diabetes &lt;- as_tibble(Age_Diabetes)\n\nprint(list(\"Summary Table of Age Proportions\"=Age_Diabetes))\n\n$`Summary Table of Age Proportions`\n# A tibble: 13 × 3\n   AgeGrp proportionNo proporitonYes\n   &lt;fct&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 1            0.0257       0.00219\n 2 2            0.0340       0.00367\n 3 3            0.0491       0.00849\n 4 4            0.0603       0.0172 \n 5 5            0.0690       0.0288 \n 6 6            0.0825       0.0482 \n 7 7            0.106        0.0860 \n 8 8            0.122        0.120  \n 9 9            0.126        0.162  \n10 10           0.118        0.186  \n11 11           0.0846       0.147  \n12 12           0.0579       0.0979 \n13 13           0.0652       0.0923 \n\nproportionNo_largestThree &lt;- sum(Age_Diabetes$proportionNo[8:10])\n\nproportionYes_largestThree &lt;- sum(Age_Diabetes$proporitonYes[9:11])\n\nprint(list(\"Cummulative Proportion of 3 largest Age groups, HasDiabetes - No\"=proportionNo_largestThree, \"Cummulative Proportion of 3 largest Age groups, HasDiabetes - Yes\"=proportionYes_largestThree))\n\n$`Cummulative Proportion of 3 largest Age groups, HasDiabetes - No`\n[1] 0.3653251\n\n$`Cummulative Proportion of 3 largest Age groups, HasDiabetes - Yes`\n[1] 0.4956554\n\n\nthat not only did the distributions shift between the age groups, but the highest three Age categories (8-10) in the “No” HasDiabetes group account for 36.51% of the group data while the highest three Age categories (9-11) in the “Yes” HasDiabetes category account for 49.32% of the group data. These shifts in proportions suggest that there is a relationship between the variables. Age appears to be suitable for prediction modeling.\nThe next variabel we’re going to summarize is Education. Review of the variable summary information\n\nprint(list(\"Summary of Education\"=summary(diabetes_df$Education)))\n\n$`Summary of Education`\n     1      2      3      4      5      6 \n   170   3975   9342  62130  69218 106643 \n\n\nshows that the responses increased in each category with the majority of observations belonging to category 6 (College 4yrs or more (College graduate)). Reviewing the bar plot of Education with respect to HasDiabetes shows\n\nEducation_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Education) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nEducation_plot &lt;- ggplot(Education_by, aes(HasDiabetes, count, fill=Education)) +\n  geom_bar(stat = \"identity\", position = position_dodge2()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), hjust=0.5) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Education by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\") +\n  coord_flip()\n\nEducation_plot\n\n\n\n\n\n\n\n\nshows that there is a definite change in the ratios of the Education gropus between the HasDiabetes groups. The “No” HasDiabetes group follows the same trend as the overall data while the “Yes” HasDiabetes group has a fairly uniform distribution between categories 4, 5, and 6. There does appear to be a relationship between the variables where we could determine there was a high probability that someone with diabetes has a high probability of having a high school diploma, GED, or higher education but the data does not lend itself to determining if a given education level can be used to predict if someone has diabetes. Education does not appear to be suitable for prediction analysis.\nThe last variable that we’re going to summarize is Income. Review of the variable summary information\n\nprint(list(\"Summary of Income\"=summary(diabetes_df$Income)))\n\n$`Summary of Income`\n    1     2     3     4     5     6     7     8 \n 9598 11542 15780 19882 25635 36184 42907 89950 \n\n\nshows that the number of responses increase as the categories increase, with the largest group earning $75K per year or more. Reviewing a bar plot of Income with respect to HasDiabetes shows\n\nIncome_by &lt;- diabetes_df |&gt;\n  group_by(HasDiabetes, Income) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'HasDiabetes'. You can override using the\n`.groups` argument.\n\nIncome_plot &lt;- ggplot(Income_by, aes(HasDiabetes, count, fill=Income)) +\n  geom_bar(stat = \"identity\", position = position_dodge2()) +\n  geom_text(aes(label = count), position = position_dodge(width=0.9), vjust=-0.25, size=3) +\n  scale_y_continuous(labels = comma) +\n  labs(title=\"Bar Plot of Income by HasDiabetes\", x=\"Has Diabetes\", y= \"# of PTs\")\n\nIncome_plot\n\n\n\n\n\n\n\n\nthat there is a shift in proportions of Income group categories between the HasDiabetes groups. The Income groups with “No” HasDiabetes responses appear to have a quadratic relationship with a large positive slope between groups 7 and 8. The Income groups with “Yes” HasDiabetes responses appear to have a linear relationship with a small positive slope. There appears to be a relationship between the variables where a person with diabetes has a greater probability of earning less than $75K pe year, but that is not what we’re trying to predict. Income does not appear to be suitable for prediction modeling.\nNow that we have identified the variables with relationships to HasDiabetes, we will consider if they are independent from one another. Review of a heat plot of HasDiabetes compared to the six predictor variables shows\n\ndiabetes_df_model &lt;- diabetes_df |&gt;\n  select(HasDiabetes, HighCholesterol, BMI, GeneralHealth, BadMentalHealth, BadPhysicalHealth, Age) \n\nindependenceCheck &lt;- model.matrix(~., \n             data=diabetes_df_model) |&gt;\n  cor(use=\"pairwise.complete.obs\") |&gt;\n  ggcorrplot(title=\"Heat Plot of Predictor Variable Correlations\", show.diag=FALSE, type=\"lower\", lab=TRUE, lab_size=1.5) +\n  theme(axis.text.x = element_text(size=6),\n        axis.text.y = element_text(size=6))\n\nprint(independenceCheck)\n\n\n\n\n\n\n\n\nthere are notable interactions between GeneralHealth, BadMentalHealth, and BadPhysicalHealth.\nAs GeneralHealth approaches category 5, the value of the correlation coefficient between it and BadPhysicalHealth increases to a moderate degree of correlation with maximum value of 0.49. GeneralHealth categories 2 through 4 all have weak correlation coefficients with BadPhysicalHealth ranging from -0.23 to 0.31.\nAs GeneralHealth approaches category 5, the value of the correlation coefficient between it and BadMentalHealth increases to a weak degree of correlation with maximum value of 0.26. GeneralHealth categories 2 through 4 also have weak correlation coefficients with BadMentalHealth ranging from -0.13 to 0.17.\nBadMentalHealth and BadPhysicalHealth have a moderate degree of correlation with a correlation coefficient of 0.35.\nBecause BadPhysicalHealth greatly influences both GeneralHealth and BadMentalHealth, because BadMentalHealth also influences GeneralHealth, and because BadPhysicalHealth has the highest correlation coefficient with HasDiabetes, it would be reasonable to remove GeneralHealth and BadMentalHealth from being used as predictor variables due to their dependence on BadPhysicalHealth.\nThe remaining combinations of variable groups all have weak correlation coefficients in the -0.30 to 0.30 range. Each of these variables appear to be independent from one another.\nNow that we have completed our EDA, we will use the above information in our Final Project Modeling"
  },
  {
    "objectID": "FinalProjectModeling.html",
    "href": "FinalProjectModeling.html",
    "title": "Final Project",
    "section": "",
    "text": "The diabetes binary health indicators file is a subset of 22 variables and 253,690 responses to questions asked of as part of the Behavioral Risk Factor Surveillance System (BHFSS) survey conducted in 2015.\nIn our EDA we identified that the variables of HighBP, HighCholesterol, BMI, GeneralHealth, BadMentalHealth, BadPhysicalHealth, and Age each had a relationship with HasDiabetes that could be used to predict the outcome of HasDiabetes. Our EDA identified that the variables Stroke, HeartDiseaseorAttack, PhysActivity, ConsumesFruits, ConsumesVeggies, HeavyAlcoholUse, ExpensiveTreatment, DifficultyWalking, and Income each had a relationship with HasDiabetes where HasDiabetes would be a predictor variable for them. This is not what we’re tyring to model so the varaibels will be excluded. We also identified that variables HasHealthcare, and Education did not have a distinguishable relationship with HasDiabetes. BMI appeared to be the only variable with outliers. These were removed from the data set before conducting EDA on the remaining variables.\nThe goal of our modeling is to predict HasDiabetes using the caret package and logLoss as the metric for evaluation of how well our models predict the actual value of **HasDiabetes*. LogLoss is the negative average of the natural log of each observation’s difference from the predicted probability. LogLoss provides a metric for determining how much error exists in a classification prediction model in a similar way MSE determines how much error exists in a linear regression prediction model. What makes LogLoss a preferred method of comparison over model accuracy is that LogLoss takes predicted probability into account when determining average error in a classification prediction model while accuracy only accounts for the proportion of correct/incorrect predictions created by the model.\nFirst, lets establish the library of functions we will use to split our data set and generate our prediction models.\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(caTools)\nlibrary(stepPlr)\nlibrary(ranger)\nlibrary(e1071)\n\nNext, we will read in the data file, remove variables identified as not suitable for prediction modeling during our EDA, remove outliers, update variable names, ensure the classification variables are changed to factors, and recode the factor variables for use in the prediction modeling functions . The resulting data frame is\n\n# Read in the raw data file, select variables for modeling, set-up variable \n# names, remove outliers. This is done to ensure the modeling page works\n# independent of the EDA page.  \n\ndiabetes_df_model &lt;- read.csv(\"./FinalProjectRawData/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\ndiabetes_df_model &lt;- as_tibble(diabetes_df_model)\n\ndiabetes_df_model &lt;- diabetes_df_model |&gt;\n  select(Diabetes_binary, HighChol, BMI, PhysHlth, Age) |&gt;\n  filter(BMI &gt;= 13.5) |&gt;\n  filter(BMI &lt;= 50 ) |&gt;\n  dplyr::rename(\"HasDiabetes\" = Diabetes_binary, \n         \"HighCholesterol\" = HighChol, \n         \"BadPhysicalHealth\" = PhysHlth) |&gt;\n  dplyr::mutate(across(c(1, 2, 5), factor))\n\ndiabetes_df_model$HasDiabetes &lt;- fct_recode(diabetes_df_model$HasDiabetes, No = \"0\", Yes = \"1\")\ndiabetes_df_model$HighCholesterol &lt;- fct_recode(diabetes_df_model$HighCholesterol, No = \"0\", Yes = \"1\")\ndiabetes_df_model$Age &lt;- fct_recode(diabetes_df_model$Age, Age1=\"1\", Age2=\"2\", Age3=\"3\", Age4=\"4\", Age5=\"5\", Age6=\"6\", Age7=\"7\", Age8=\"8\", Age9=\"9\", Age10=\"10\", Age11=\"11\", Age12=\"12\", Age13=\"13\")\n   \nstr(diabetes_df_model)\n\ntibble [251,478 × 5] (S3: tbl_df/tbl/data.frame)\n $ HasDiabetes      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighCholesterol  : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ BMI              : num [1:251478] 40 25 28 27 24 25 30 25 30 24 ...\n $ BadPhysicalHealth: num [1:251478] 15 0 30 0 0 2 14 0 30 0 ...\n $ Age              : Factor w/ 13 levels \"Age1\",\"Age2\",..: 9 7 9 11 11 10 9 11 9 8 ...\n\n\nWe see from the structure of our subset that the data frame was converted properly and we’re ready to continue preparing the data for prediction modeling. Next we will split the data frame into training and test subsets. We will use 70% of the data set for the training model and 30% of the data set for testing our model.\n\n# 70/30 split for training/testing subsets\n# Use set.seed() to make things reproducible \n\nset.seed(1)\n\n# Create the model index for partitioning the data \nmodelingIndex &lt;- createDataPartition(diabetes_df_model$HasDiabetes, p=.7, list=FALSE)\n\n# Create the training set\nmodelingTrain &lt;- diabetes_df_model[modelingIndex, ]\n\n# Create the test set\nmodelingTest &lt;- diabetes_df_model[-modelingIndex, ]\n\n# Output the dimensions to ensure the data subsets populated correctly\nprint(list(\"Traning Data\"=dim(modelingTrain), \"Test Data\"=dim(modelingTest)))\n\n$`Traning Data`\n[1] 176035      5\n\n$`Test Data`\n[1] 75443     5\n\n\nReview of the dimensions of both subsets shows that we successfully split the data and are ready to start generating our prediction models.\n\n\n\nThe first prediction model type we’re going to generate is a logistic regression model. Logistic regression is the estimated probability of an event occurring when considering other variables involved. It is widely used in prediction modeling when a dependent variable of interest has binary outcomes. For our prediction model, we will be the estimating the probability of a PT having diabetes with consideration of the variables HighCholesterol, BMI, BadPhysicalHealth, and Age to predict a “Yes” response in the HasDiabetes variable.\nThere are several different logistic regression models (LRMs) that are in the caret package. Because of this, we are going to select three types of LRMs and compare them to determine which one we will use for comparison against a Classification Tree and Random Forest prediction models that we will generate later in this report. For each logistic regression model, we used repeated 10-fold cross validation with the number of repeats being 5 in order to give us several values to .\nThe first logistic regression model we’re going to train is the Boosted Logistic Regression Model.\n\nset.seed(1)\n\nboostedLogistic &lt;- train(HasDiabetes ~ HighCholesterol + BMI + BadPhysicalHealth + Age , \n                        data=modelingTrain,\n                        method=\"LogitBoost\",\n                        metric=\"logLoss\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss),\n                        tuneGrid = expand.grid(nIter = seq(1, 30, by = 1))\n                        )\n\nprint(boostedLogistic)\n\nBoosted Logistic Regression \n\n176035 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (15), scaled (15) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 140828, 140829, 140828, 140827, 140828 \nResampling results across tuning parameters:\n\n  nIter  logLoss  \n   1     0.4510127\n   2     0.3695666\n   3     0.3887616\n   4     0.3585300\n   5     0.4102161\n   6     0.3557310\n   7     0.4002803\n   8     0.3480972\n   9     0.4059628\n  10     0.3559596\n  11     0.4003597\n  12     0.3454173\n  13     0.4101367\n  14     0.3584109\n  15     0.3961064\n  16     0.3456459\n  17     0.4102161\n  18     0.3557310\n  19     0.4002803\n  20     0.3480972\n  21     0.4059628\n  22     0.3559596\n  23     0.4003597\n  24     0.3454173\n  25     0.4101367\n  26     0.3584109\n  27     0.3961064\n  28     0.3456459\n  29     0.4102161\n  30     0.3557310\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was nIter = 12.\n\nplot(boostedLogistic)\n\n\n\n\n\n\n\n\nThe second logistic regression model we’re going to train is the Bayesian Generalized Linear Model.\n\nset.seed(1)\n\nbayesianLogistic &lt;- train(HasDiabetes~., \n                        data=modelingTrain,\n                        method=\"bayesglm\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss)\n                        )\n\nprint(bayesianLogistic)\n\nBayesian Generalized Linear Model \n\n176035 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (15), scaled (15) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 140828, 140829, 140828, 140827, 140828 \nResampling results:\n\n  logLoss  \n  0.3391554\n\n\nThere were no results to plot of this model because there are no tuning parameters for the bayesglm function.\nThe third logistic regression model we’re going to train is the Generalized Linear Model.\n\nset.seed(1)\n\ngeneralizedLogistic &lt;- train(HasDiabetes~., \n                        data=modelingTrain,\n                        method=\"glm\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss)\n                        )\n\nprint(generalizedLogistic)\n\nGeneralized Linear Model \n\n176035 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (15), scaled (15) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 140828, 140829, 140828, 140827, 140828 \nResampling results:\n\n  logLoss \n  0.339156\n\n\nThere were no results to plot of this model because there are no tuning parameters for the glm function.\nNow that we have the logLoss established for each logistic regression, comparison of them shows\n\nprint(list(\"Comparison of Logistic Regression Model LogLoss\"=t(data.frame(\"GLM\"=generalizedLogistic$results$logLoss, \"BayesianGLM\"=bayesianLogistic$results$logLoss, \"Boosted\"=boostedLogistic$results$logLoss[12]))))\n\n$`Comparison of Logistic Regression Model LogLoss`\n                 [,1]\nGLM         0.3391560\nBayesianGLM 0.3391554\nBoosted     0.3454173\n\n\nthat the Bayesian Generalized Logistic Model (GLM) performed the best with a LogLoss of 0.3391554. We will use this model for overall comparison between the different prediction model types.\n\n\n\n\nProvide a reasonably thorough explanation of what a classification tree model is and why we might try to use it.\n\nWe will use rpart to fit the training data to a classification tree with varying values for the complexity parameter (cp) ranging from “0” to “0.1” and then choose the best model generated.\n\nset.seed(1)\n\ncartTree &lt;- train(HasDiabetes~., \n                        data=modelingTrain,\n                        method=\"rpart\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss),\n                        tuneGrid = data.frame(cp = seq(0, 0.1, by = 0.001))\n                        )\n\nprint(cartTree)\n\nCART \n\n176035 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (15), scaled (15) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 140828, 140829, 140828, 140827, 140828 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3564431\n  0.001  0.3702911\n  0.002  0.3827386\n  0.003  0.4008598\n  0.004  0.4008598\n  0.005  0.4008598\n  0.006  0.4008598\n  0.007  0.4008598\n  0.008  0.4008598\n  0.009  0.4008598\n  0.010  0.4008598\n  0.011  0.4008598\n  0.012  0.4008598\n  0.013  0.4008598\n  0.014  0.4008598\n  0.015  0.4008598\n  0.016  0.4008598\n  0.017  0.4008598\n  0.018  0.4008598\n  0.019  0.4008598\n  0.020  0.4008598\n  0.021  0.4008598\n  0.022  0.4008598\n  0.023  0.4008598\n  0.024  0.4008598\n  0.025  0.4008598\n  0.026  0.4008598\n  0.027  0.4008598\n  0.028  0.4008598\n  0.029  0.4008598\n  0.030  0.4008598\n  0.031  0.4008598\n  0.032  0.4008598\n  0.033  0.4008598\n  0.034  0.4008598\n  0.035  0.4008598\n  0.036  0.4008598\n  0.037  0.4008598\n  0.038  0.4008598\n  0.039  0.4008598\n  0.040  0.4008598\n  0.041  0.4008598\n  0.042  0.4008598\n  0.043  0.4008598\n  0.044  0.4008598\n  0.045  0.4008598\n  0.046  0.4008598\n  0.047  0.4008598\n  0.048  0.4008598\n  0.049  0.4008598\n  0.050  0.4008598\n  0.051  0.4008598\n  0.052  0.4008598\n  0.053  0.4008598\n  0.054  0.4008598\n  0.055  0.4008598\n  0.056  0.4008598\n  0.057  0.4008598\n  0.058  0.4008598\n  0.059  0.4008598\n  0.060  0.4008598\n  0.061  0.4008598\n  0.062  0.4008598\n  0.063  0.4008598\n  0.064  0.4008598\n  0.065  0.4008598\n  0.066  0.4008598\n  0.067  0.4008598\n  0.068  0.4008598\n  0.069  0.4008598\n  0.070  0.4008598\n  0.071  0.4008598\n  0.072  0.4008598\n  0.073  0.4008598\n  0.074  0.4008598\n  0.075  0.4008598\n  0.076  0.4008598\n  0.077  0.4008598\n  0.078  0.4008598\n  0.079  0.4008598\n  0.080  0.4008598\n  0.081  0.4008598\n  0.082  0.4008598\n  0.083  0.4008598\n  0.084  0.4008598\n  0.085  0.4008598\n  0.086  0.4008598\n  0.087  0.4008598\n  0.088  0.4008598\n  0.089  0.4008598\n  0.090  0.4008598\n  0.091  0.4008598\n  0.092  0.4008598\n  0.093  0.4008598\n  0.094  0.4008598\n  0.095  0.4008598\n  0.096  0.4008598\n  0.097  0.4008598\n  0.098  0.4008598\n  0.099  0.4008598\n  0.100  0.4008598\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\nplot(cartTree)\n\n\n\n\n\n\n\n\nReview of the data and chart shows that the initial complexity parameter of “0.000” had the lowest logLoss with a value of 0.3564431.\n\n\n\n\nProvide a reasonably thorough explanation of what a classification tree model is and why we might use it (focus on differences between it and a classification tree).\n\nRanger is a computationally faster method over random forest. It has tuning parameters of mtry, splitrule, and min.node.size that we will use to determine the best model for comparison. While it is standard to take the square root of the number of predictor variables to determine our mtry value, we only have four so we will set mtry to compare use of 1, 2, and 3 of our predictor variables. We will not uses all four predictor variables because that is the same as a bagging model, which defeats the purpose of generating a Random Tree model. We will set splitrule to “extratrees” to replicate the random forest effect in ranger. We will compare min.node.size values of “1”, “10”, “100”, and “500” to determine which one is more accurate while saving computation time. The larger the value of min.node.size, the less time it takes to generate the trees. Due to the large number of observations in our training data set, we are trading off .\n\nset.seed(1)\n\nrandomForest &lt;- train(HasDiabetes~.,\n                      data=modelingTrain,\n                      method=\"ranger\",\n                      preProcess=c(\"center\", \"scale\"),\n                      trControl = trainControl(method = \"cv\",\n                                               number = 5,\n                                               classProbs = TRUE,\n                                               summaryFunction = mnLogLoss),\n                      tuneGrid = expand.grid(mtry = c(1,2,3),\n                                             splitrule=\"extratrees\",\n                                             min.node.size=c(1, 10, 100, 500)),\n                      verbose = FALSE\n                                              \n                        )\n\nprint(randomForest)\n\nRandom Forest \n\n176035 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (15), scaled (15) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 140828, 140829, 140828, 140827, 140828 \nResampling results across tuning parameters:\n\n  mtry  min.node.size  logLoss  \n  1       1            0.3743224\n  1      10            0.3745156\n  1     100            0.3743105\n  1     500            0.3742334\n  2       1            0.3544900\n  2      10            0.3551585\n  2     100            0.3546264\n  2     500            0.3553097\n  3       1            0.3466373\n  3      10            0.3464193\n  3     100            0.3464740\n  3     500            0.3467557\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 3, splitrule = extratrees\n and min.node.size = 10.\n\nplot(randomForest)\n\n\n\n\n\n\n\n\nFrom the data we see that increasing the number of predictor variables greatly reduced the amount of logLoss in the model, with mtry of 3 and min.node.size being optimal for regression comparison.\n\n\n\n\nCompare the three best models and “declare an overall winner”\n\nIn order to compare the three best models, we will need to run the training models using the best fit tuning parameters identified earlier in this report.\n\nset.seed(1)\n\n#---------- produce models using identified best fit tuning params ----------#\n\n# selected logistic regression model\nbayesianLogisticComp &lt;- train(HasDiabetes~., \n                        data=modelingTrain,\n                        method=\"bayesglm\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss)\n                        )\n\n# selected classification tree model\ncartTreeComp &lt;- train(HasDiabetes~., \n                        data=modelingTrain,\n                        method=\"rpart\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss),\n                        tuneGrid = data.frame(cp = 0)\n                        )\n\n# selected random forest model\nrandomForestComp &lt;- train(HasDiabetes~.,\n                      data=modelingTrain,\n                      method=\"ranger\",\n                      preProcess=c(\"center\", \"scale\"),\n                      trControl = trainControl(method = \"cv\",\n                                               number = 5,\n                                               classProbs = TRUE,\n                                               summaryFunction = mnLogLoss),\n                      tuneGrid = expand.grid(mtry = 3,\n                                             splitrule=\"extratrees\",\n                                             min.node.size=10),\n                      verbose = FALSE\n                                              \n                        )\n\nNext we will generate the model predictions, accuracy, and compare the results to one another.\n\nset.seed(1) \n\n#---------- predict and validate each model using confusion matrix ----------#\n\n# generate model predictions\nbayesianLogisticComp_train_pred &lt;- predict(bayesianLogisticComp, newdata = modelingTest)\ncartTreeComp_train_pred &lt;- predict(cartTreeComp, newdata = modelingTest)\nrandomForestComp_train_pred &lt;- predict(randomForestComp, newdata = modelingTest)\n\n\n# validate the accuracy of the predicton agains the test data set\nbayesianLogisticComp_accuracy &lt;- confusionMatrix(bayesianLogisticComp_train_pred, modelingTest$HasDiabetes)\n\ncartTreeComp_accuracy &lt;- confusionMatrix(cartTreeComp_train_pred, modelingTest$HasDiabetes)\n\nrandomForestComp_accuracy &lt;- confusionMatrix(randomForestComp_train_pred, modelingTest$HasDiabetes)\n\n\n# frame the results for comparison\nmodelComps &lt;- t(data.frame(\"BayesianGLM\"=bayesianLogisticComp_accuracy$overall[1], \"RPart\"=cartTreeComp_accuracy$overall[1], \"RandomForest\"=randomForestComp_accuracy$overall[1]))\n\n# output the results\nprint(modelComps)\n\n              Accuracy\nBayesianGLM  0.8638310\nRPart        0.8600268\nRandomForest 0.8623729\n\n\nReview of the accuracy for each model when predicting the outcome of HasDiabetes shows that all three models have an accuracy rate of 86% or higher. The BayesianGLM model has the highest level of accuracy at just over 86.383% and will be selected for use in the API.\n\n\n\nInspiration for approaching the Random Forest model training came from several of my fellow students and Professor Post in ST 558-651 SUM1 2024. Their contributions to the discussion forum topics related to the final project were paramount to my concept development in how to approach generating the Random Forest models in the most time efficient manner."
  },
  {
    "objectID": "FinalProjectModeling.html#diabetes-health-indicators---modeling",
    "href": "FinalProjectModeling.html#diabetes-health-indicators---modeling",
    "title": "Final Project",
    "section": "",
    "text": "The diabetes binary health indicators file is a subset of 22 variables and 253,690 responses to questions asked of as part of the Behavioral Risk Factor Surveillance System (BHFSS) survey conducted in 2015.\nIn our EDA we identified that the variables of HighBP, HighCholesterol, BMI, GeneralHealth, BadMentalHealth, BadPhysicalHealth, and Age each had a relationship with HasDiabetes that could be used to predict the outcome of HasDiabetes. Our EDA identified that the variables Stroke, HeartDiseaseorAttack, PhysActivity, ConsumesFruits, ConsumesVeggies, HeavyAlcoholUse, ExpensiveTreatment, DifficultyWalking, and Income each had a relationship with HasDiabetes where HasDiabetes would be a predictor variable for them. This is not what we’re tyring to model so the varaibels will be excluded. We also identified that variables HasHealthcare, and Education did not have a distinguishable relationship with HasDiabetes. BMI appeared to be the only variable with outliers. These were removed from the data set before conducting EDA on the remaining variables.\nThe goal of our modeling is to predict HasDiabetes using the caret package and logLoss as the metric for evaluation of how well our models predict the actual value of **HasDiabetes*. LogLoss is the negative average of the natural log of each observation’s difference from the predicted probability. LogLoss provides a metric for determining how much error exists in a classification prediction model in a similar way MSE determines how much error exists in a linear regression prediction model. What makes LogLoss a preferred method of comparison over model accuracy is that LogLoss takes predicted probability into account when determining average error in a classification prediction model while accuracy only accounts for the proportion of correct/incorrect predictions created by the model.\nFirst, lets establish the library of functions we will use to split our data set and generate our prediction models.\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(caTools)\nlibrary(stepPlr)\nlibrary(ranger)\nlibrary(e1071)\n\nNext, we will read in the data file, remove variables identified as not suitable for prediction modeling during our EDA, remove outliers, update variable names, ensure the classification variables are changed to factors, and recode the factor variables for use in the prediction modeling functions . The resulting data frame is\n\n# Read in the raw data file, select variables for modeling, set-up variable \n# names, remove outliers. This is done to ensure the modeling page works\n# independent of the EDA page.  \n\ndiabetes_df_model &lt;- read.csv(\"./FinalProjectRawData/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\ndiabetes_df_model &lt;- as_tibble(diabetes_df_model)\n\ndiabetes_df_model &lt;- diabetes_df_model |&gt;\n  select(Diabetes_binary, HighChol, BMI, PhysHlth, Age) |&gt;\n  filter(BMI &gt;= 13.5) |&gt;\n  filter(BMI &lt;= 50 ) |&gt;\n  dplyr::rename(\"HasDiabetes\" = Diabetes_binary, \n         \"HighCholesterol\" = HighChol, \n         \"BadPhysicalHealth\" = PhysHlth) |&gt;\n  dplyr::mutate(across(c(1, 2, 5), factor))\n\ndiabetes_df_model$HasDiabetes &lt;- fct_recode(diabetes_df_model$HasDiabetes, No = \"0\", Yes = \"1\")\ndiabetes_df_model$HighCholesterol &lt;- fct_recode(diabetes_df_model$HighCholesterol, No = \"0\", Yes = \"1\")\ndiabetes_df_model$Age &lt;- fct_recode(diabetes_df_model$Age, Age1=\"1\", Age2=\"2\", Age3=\"3\", Age4=\"4\", Age5=\"5\", Age6=\"6\", Age7=\"7\", Age8=\"8\", Age9=\"9\", Age10=\"10\", Age11=\"11\", Age12=\"12\", Age13=\"13\")\n   \nstr(diabetes_df_model)\n\ntibble [251,478 × 5] (S3: tbl_df/tbl/data.frame)\n $ HasDiabetes      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighCholesterol  : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ BMI              : num [1:251478] 40 25 28 27 24 25 30 25 30 24 ...\n $ BadPhysicalHealth: num [1:251478] 15 0 30 0 0 2 14 0 30 0 ...\n $ Age              : Factor w/ 13 levels \"Age1\",\"Age2\",..: 9 7 9 11 11 10 9 11 9 8 ...\n\n\nWe see from the structure of our subset that the data frame was converted properly and we’re ready to continue preparing the data for prediction modeling. Next we will split the data frame into training and test subsets. We will use 70% of the data set for the training model and 30% of the data set for testing our model.\n\n# 70/30 split for training/testing subsets\n# Use set.seed() to make things reproducible \n\nset.seed(1)\n\n# Create the model index for partitioning the data \nmodelingIndex &lt;- createDataPartition(diabetes_df_model$HasDiabetes, p=.7, list=FALSE)\n\n# Create the training set\nmodelingTrain &lt;- diabetes_df_model[modelingIndex, ]\n\n# Create the test set\nmodelingTest &lt;- diabetes_df_model[-modelingIndex, ]\n\n# Output the dimensions to ensure the data subsets populated correctly\nprint(list(\"Traning Data\"=dim(modelingTrain), \"Test Data\"=dim(modelingTest)))\n\n$`Traning Data`\n[1] 176035      5\n\n$`Test Data`\n[1] 75443     5\n\n\nReview of the dimensions of both subsets shows that we successfully split the data and are ready to start generating our prediction models.\n\n\n\nThe first prediction model type we’re going to generate is a logistic regression model. Logistic regression is the estimated probability of an event occurring when considering other variables involved. It is widely used in prediction modeling when a dependent variable of interest has binary outcomes. For our prediction model, we will be the estimating the probability of a PT having diabetes with consideration of the variables HighCholesterol, BMI, BadPhysicalHealth, and Age to predict a “Yes” response in the HasDiabetes variable.\nThere are several different logistic regression models (LRMs) that are in the caret package. Because of this, we are going to select three types of LRMs and compare them to determine which one we will use for comparison against a Classification Tree and Random Forest prediction models that we will generate later in this report. For each logistic regression model, we used repeated 10-fold cross validation with the number of repeats being 5 in order to give us several values to .\nThe first logistic regression model we’re going to train is the Boosted Logistic Regression Model.\n\nset.seed(1)\n\nboostedLogistic &lt;- train(HasDiabetes ~ HighCholesterol + BMI + BadPhysicalHealth + Age , \n                        data=modelingTrain,\n                        method=\"LogitBoost\",\n                        metric=\"logLoss\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss),\n                        tuneGrid = expand.grid(nIter = seq(1, 30, by = 1))\n                        )\n\nprint(boostedLogistic)\n\nBoosted Logistic Regression \n\n176035 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (15), scaled (15) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 140828, 140829, 140828, 140827, 140828 \nResampling results across tuning parameters:\n\n  nIter  logLoss  \n   1     0.4510127\n   2     0.3695666\n   3     0.3887616\n   4     0.3585300\n   5     0.4102161\n   6     0.3557310\n   7     0.4002803\n   8     0.3480972\n   9     0.4059628\n  10     0.3559596\n  11     0.4003597\n  12     0.3454173\n  13     0.4101367\n  14     0.3584109\n  15     0.3961064\n  16     0.3456459\n  17     0.4102161\n  18     0.3557310\n  19     0.4002803\n  20     0.3480972\n  21     0.4059628\n  22     0.3559596\n  23     0.4003597\n  24     0.3454173\n  25     0.4101367\n  26     0.3584109\n  27     0.3961064\n  28     0.3456459\n  29     0.4102161\n  30     0.3557310\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was nIter = 12.\n\nplot(boostedLogistic)\n\n\n\n\n\n\n\n\nThe second logistic regression model we’re going to train is the Bayesian Generalized Linear Model.\n\nset.seed(1)\n\nbayesianLogistic &lt;- train(HasDiabetes~., \n                        data=modelingTrain,\n                        method=\"bayesglm\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss)\n                        )\n\nprint(bayesianLogistic)\n\nBayesian Generalized Linear Model \n\n176035 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (15), scaled (15) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 140828, 140829, 140828, 140827, 140828 \nResampling results:\n\n  logLoss  \n  0.3391554\n\n\nThere were no results to plot of this model because there are no tuning parameters for the bayesglm function.\nThe third logistic regression model we’re going to train is the Generalized Linear Model.\n\nset.seed(1)\n\ngeneralizedLogistic &lt;- train(HasDiabetes~., \n                        data=modelingTrain,\n                        method=\"glm\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss)\n                        )\n\nprint(generalizedLogistic)\n\nGeneralized Linear Model \n\n176035 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (15), scaled (15) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 140828, 140829, 140828, 140827, 140828 \nResampling results:\n\n  logLoss \n  0.339156\n\n\nThere were no results to plot of this model because there are no tuning parameters for the glm function.\nNow that we have the logLoss established for each logistic regression, comparison of them shows\n\nprint(list(\"Comparison of Logistic Regression Model LogLoss\"=t(data.frame(\"GLM\"=generalizedLogistic$results$logLoss, \"BayesianGLM\"=bayesianLogistic$results$logLoss, \"Boosted\"=boostedLogistic$results$logLoss[12]))))\n\n$`Comparison of Logistic Regression Model LogLoss`\n                 [,1]\nGLM         0.3391560\nBayesianGLM 0.3391554\nBoosted     0.3454173\n\n\nthat the Bayesian Generalized Logistic Model (GLM) performed the best with a LogLoss of 0.3391554. We will use this model for overall comparison between the different prediction model types.\n\n\n\n\nProvide a reasonably thorough explanation of what a classification tree model is and why we might try to use it.\n\nWe will use rpart to fit the training data to a classification tree with varying values for the complexity parameter (cp) ranging from “0” to “0.1” and then choose the best model generated.\n\nset.seed(1)\n\ncartTree &lt;- train(HasDiabetes~., \n                        data=modelingTrain,\n                        method=\"rpart\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss),\n                        tuneGrid = data.frame(cp = seq(0, 0.1, by = 0.001))\n                        )\n\nprint(cartTree)\n\nCART \n\n176035 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (15), scaled (15) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 140828, 140829, 140828, 140827, 140828 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3564431\n  0.001  0.3702911\n  0.002  0.3827386\n  0.003  0.4008598\n  0.004  0.4008598\n  0.005  0.4008598\n  0.006  0.4008598\n  0.007  0.4008598\n  0.008  0.4008598\n  0.009  0.4008598\n  0.010  0.4008598\n  0.011  0.4008598\n  0.012  0.4008598\n  0.013  0.4008598\n  0.014  0.4008598\n  0.015  0.4008598\n  0.016  0.4008598\n  0.017  0.4008598\n  0.018  0.4008598\n  0.019  0.4008598\n  0.020  0.4008598\n  0.021  0.4008598\n  0.022  0.4008598\n  0.023  0.4008598\n  0.024  0.4008598\n  0.025  0.4008598\n  0.026  0.4008598\n  0.027  0.4008598\n  0.028  0.4008598\n  0.029  0.4008598\n  0.030  0.4008598\n  0.031  0.4008598\n  0.032  0.4008598\n  0.033  0.4008598\n  0.034  0.4008598\n  0.035  0.4008598\n  0.036  0.4008598\n  0.037  0.4008598\n  0.038  0.4008598\n  0.039  0.4008598\n  0.040  0.4008598\n  0.041  0.4008598\n  0.042  0.4008598\n  0.043  0.4008598\n  0.044  0.4008598\n  0.045  0.4008598\n  0.046  0.4008598\n  0.047  0.4008598\n  0.048  0.4008598\n  0.049  0.4008598\n  0.050  0.4008598\n  0.051  0.4008598\n  0.052  0.4008598\n  0.053  0.4008598\n  0.054  0.4008598\n  0.055  0.4008598\n  0.056  0.4008598\n  0.057  0.4008598\n  0.058  0.4008598\n  0.059  0.4008598\n  0.060  0.4008598\n  0.061  0.4008598\n  0.062  0.4008598\n  0.063  0.4008598\n  0.064  0.4008598\n  0.065  0.4008598\n  0.066  0.4008598\n  0.067  0.4008598\n  0.068  0.4008598\n  0.069  0.4008598\n  0.070  0.4008598\n  0.071  0.4008598\n  0.072  0.4008598\n  0.073  0.4008598\n  0.074  0.4008598\n  0.075  0.4008598\n  0.076  0.4008598\n  0.077  0.4008598\n  0.078  0.4008598\n  0.079  0.4008598\n  0.080  0.4008598\n  0.081  0.4008598\n  0.082  0.4008598\n  0.083  0.4008598\n  0.084  0.4008598\n  0.085  0.4008598\n  0.086  0.4008598\n  0.087  0.4008598\n  0.088  0.4008598\n  0.089  0.4008598\n  0.090  0.4008598\n  0.091  0.4008598\n  0.092  0.4008598\n  0.093  0.4008598\n  0.094  0.4008598\n  0.095  0.4008598\n  0.096  0.4008598\n  0.097  0.4008598\n  0.098  0.4008598\n  0.099  0.4008598\n  0.100  0.4008598\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\nplot(cartTree)\n\n\n\n\n\n\n\n\nReview of the data and chart shows that the initial complexity parameter of “0.000” had the lowest logLoss with a value of 0.3564431.\n\n\n\n\nProvide a reasonably thorough explanation of what a classification tree model is and why we might use it (focus on differences between it and a classification tree).\n\nRanger is a computationally faster method over random forest. It has tuning parameters of mtry, splitrule, and min.node.size that we will use to determine the best model for comparison. While it is standard to take the square root of the number of predictor variables to determine our mtry value, we only have four so we will set mtry to compare use of 1, 2, and 3 of our predictor variables. We will not uses all four predictor variables because that is the same as a bagging model, which defeats the purpose of generating a Random Tree model. We will set splitrule to “extratrees” to replicate the random forest effect in ranger. We will compare min.node.size values of “1”, “10”, “100”, and “500” to determine which one is more accurate while saving computation time. The larger the value of min.node.size, the less time it takes to generate the trees. Due to the large number of observations in our training data set, we are trading off .\n\nset.seed(1)\n\nrandomForest &lt;- train(HasDiabetes~.,\n                      data=modelingTrain,\n                      method=\"ranger\",\n                      preProcess=c(\"center\", \"scale\"),\n                      trControl = trainControl(method = \"cv\",\n                                               number = 5,\n                                               classProbs = TRUE,\n                                               summaryFunction = mnLogLoss),\n                      tuneGrid = expand.grid(mtry = c(1,2,3),\n                                             splitrule=\"extratrees\",\n                                             min.node.size=c(1, 10, 100, 500)),\n                      verbose = FALSE\n                                              \n                        )\n\nprint(randomForest)\n\nRandom Forest \n\n176035 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (15), scaled (15) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 140828, 140829, 140828, 140827, 140828 \nResampling results across tuning parameters:\n\n  mtry  min.node.size  logLoss  \n  1       1            0.3743224\n  1      10            0.3745156\n  1     100            0.3743105\n  1     500            0.3742334\n  2       1            0.3544900\n  2      10            0.3551585\n  2     100            0.3546264\n  2     500            0.3553097\n  3       1            0.3466373\n  3      10            0.3464193\n  3     100            0.3464740\n  3     500            0.3467557\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 3, splitrule = extratrees\n and min.node.size = 10.\n\nplot(randomForest)\n\n\n\n\n\n\n\n\nFrom the data we see that increasing the number of predictor variables greatly reduced the amount of logLoss in the model, with mtry of 3 and min.node.size being optimal for regression comparison.\n\n\n\n\nCompare the three best models and “declare an overall winner”\n\nIn order to compare the three best models, we will need to run the training models using the best fit tuning parameters identified earlier in this report.\n\nset.seed(1)\n\n#---------- produce models using identified best fit tuning params ----------#\n\n# selected logistic regression model\nbayesianLogisticComp &lt;- train(HasDiabetes~., \n                        data=modelingTrain,\n                        method=\"bayesglm\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss)\n                        )\n\n# selected classification tree model\ncartTreeComp &lt;- train(HasDiabetes~., \n                        data=modelingTrain,\n                        method=\"rpart\",\n                        preProcess=c(\"center\", \"scale\"),\n                        trControl = trainControl(method = \"cv\",\n                                                 number = 5,\n                                                 classProbs = TRUE,\n                                                 summaryFunction = mnLogLoss),\n                        tuneGrid = data.frame(cp = 0)\n                        )\n\n# selected random forest model\nrandomForestComp &lt;- train(HasDiabetes~.,\n                      data=modelingTrain,\n                      method=\"ranger\",\n                      preProcess=c(\"center\", \"scale\"),\n                      trControl = trainControl(method = \"cv\",\n                                               number = 5,\n                                               classProbs = TRUE,\n                                               summaryFunction = mnLogLoss),\n                      tuneGrid = expand.grid(mtry = 3,\n                                             splitrule=\"extratrees\",\n                                             min.node.size=10),\n                      verbose = FALSE\n                                              \n                        )\n\nNext we will generate the model predictions, accuracy, and compare the results to one another.\n\nset.seed(1) \n\n#---------- predict and validate each model using confusion matrix ----------#\n\n# generate model predictions\nbayesianLogisticComp_train_pred &lt;- predict(bayesianLogisticComp, newdata = modelingTest)\ncartTreeComp_train_pred &lt;- predict(cartTreeComp, newdata = modelingTest)\nrandomForestComp_train_pred &lt;- predict(randomForestComp, newdata = modelingTest)\n\n\n# validate the accuracy of the predicton agains the test data set\nbayesianLogisticComp_accuracy &lt;- confusionMatrix(bayesianLogisticComp_train_pred, modelingTest$HasDiabetes)\n\ncartTreeComp_accuracy &lt;- confusionMatrix(cartTreeComp_train_pred, modelingTest$HasDiabetes)\n\nrandomForestComp_accuracy &lt;- confusionMatrix(randomForestComp_train_pred, modelingTest$HasDiabetes)\n\n\n# frame the results for comparison\nmodelComps &lt;- t(data.frame(\"BayesianGLM\"=bayesianLogisticComp_accuracy$overall[1], \"RPart\"=cartTreeComp_accuracy$overall[1], \"RandomForest\"=randomForestComp_accuracy$overall[1]))\n\n# output the results\nprint(modelComps)\n\n              Accuracy\nBayesianGLM  0.8638310\nRPart        0.8600268\nRandomForest 0.8623729\n\n\nReview of the accuracy for each model when predicting the outcome of HasDiabetes shows that all three models have an accuracy rate of 86% or higher. The BayesianGLM model has the highest level of accuracy at just over 86.383% and will be selected for use in the API.\n\n\n\nInspiration for approaching the Random Forest model training came from several of my fellow students and Professor Post in ST 558-651 SUM1 2024. Their contributions to the discussion forum topics related to the final project were paramount to my concept development in how to approach generating the Random Forest models in the most time efficient manner."
  }
]